{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ad80fc",
   "metadata": {},
   "source": [
    "# CLASE 5.2: MANIPULACIÓN DE DATOS EN ESTRUCTURAS DE **<font color=\"mediumorchid\">POLARS</font>**.\n",
    "---\n",
    "\n",
    "## Carga y guardado de datos en **<font color=\"mediumorchid\">Polars</font>**.\n",
    "Como ya hemos visto en todo el desarrollo de la asignatura, en general, el acceso a la información de interés propia de algún fenómeno, proceso o sistema que estamos interesados en analizar, suele provenir de archivos externos, muchas veces preprocesados por terceros. Por lo tanto, es común que hagamos uso de funciones especializadas en leer y/o acceder a tales archivos, o bien, que guarden las series y/o DataFrames que construyamos como resultado de algún análisis en nuestro computador. En **<font color=\"mediumorchid\">Polars</font>**, tales funciones se resumen en un conjunto conocido como **IO** (del inglés **Input/Output**).\n",
    "\n",
    "Antes de comentar las funciones de IO propias de **<font color=\"mediumorchid\">Polars</font>**, importaremos esta librería (junto con **<font color=\"mediumorchid\">Numpy</font>** y **<font color=\"mediumorchid\">Pandas</font>**) a fin de hacer las correspondientes comparaciones en tiempo de ejecución que, en esta sección, sí serán de enorme importancia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1107669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e45eaa",
   "metadata": {},
   "source": [
    "En esta sección, haremos uso de algunos archivos ubicados en la carpeta `datasets`, a fin de ejemplificar en primera instancia el cómo acceder a la data contenida en los mismos mediante el uso de algunas funciones de IO en **<font color=\"mediumorchid\">Polars</font>**. Partiremos, como en el caso de **<font color=\"mediumorchid\">Pandas</font>**, mostrando el acceso a archivos nativos de Microsoft Excel, a los que podremos acceder siempre por medio de la función `pl.read_excel()`. Sin embargo, para ello, será necesario instalar un motor de lectura de archivos de este tipo distinto del usado en **<font color=\"mediumorchid\">Pandas</font>**, llamado `xlsx2csv`. Para ello, mediante nuestra consola, escribimos la instrucción:\n",
    "\n",
    "    pip install xlsx2csv\n",
    "\n",
    "Y ya podemos acceder a nuestros archivos de Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df39cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceso a un archivo nativo de Excel en Polars.\n",
    "data = pl.read_excel(source=\"datasets/pillars_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f436e66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 7)\n",
      "┌────────────┬────────────┬──────┬──────────┬────────────────┬───────────┬─────────────┐\n",
      "│ x          ┆ y          ┆ z    ┆ area     ┆ frec_fracturas ┆ sigma_z   ┆ tiraje_prom │\n",
      "│ ---        ┆ ---        ┆ ---  ┆ ---      ┆ ---            ┆ ---       ┆ ---         │\n",
      "│ f64        ┆ f64        ┆ i64  ┆ f64      ┆ f64            ┆ f64       ┆ f64         │\n",
      "╞════════════╪════════════╪══════╪══════════╪════════════════╪═══════════╪═════════════╡\n",
      "│ 776.718035 ┆ 259.185135 ┆ 1125 ┆ 286.5643 ┆ 1.979          ┆ 11.126392 ┆ 122.685     │\n",
      "│ 811.143435 ┆ 263.059235 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 9.646313  ┆ 121.178     │\n",
      "│ 845.568835 ┆ 266.933335 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 10.590594 ┆ 118.942     │\n",
      "│ 879.994235 ┆ 270.807435 ┆ 1125 ┆ 286.5643 ┆ 1.75           ┆ 10.605144 ┆ 120.012     │\n",
      "│ 914.419635 ┆ 274.681535 ┆ 1125 ┆ 286.5643 ┆ 0.67           ┆ 10.059537 ┆ 122.787     │\n",
      "└────────────┴────────────┴──────┴──────────┴────────────────┴───────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras 5 filas de este DataFrame.\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babfd711",
   "metadata": {},
   "source": [
    "Como en el caso de **<font color=\"mediumorchid\">Pandas</font>**, el acceso a archivos nativos y/o creados por Microsoft Excel en **<font color=\"mediumorchid\">Polars</font>** es una tarea que consume varios recursos de memoria, y puede catalogarse como de ejecución *lenta*. Sin embargo, **<font color=\"mediumorchid\">Polars</font>** es capaz de acceder a estos archivos más rápido que **<font color=\"mediumorchid\">Pandas</font>**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a12be42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.1 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "52.6 ms ± 163 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pd.read_excel(io=\"datasets/pillars_data.xlsx\")\n",
    "%timeit pl.read_excel(source=\"datasets/pillars_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66afa7c",
   "metadata": {},
   "source": [
    "Es posible seleccionar cualquier hoja en un libro de Excel accediendo al archivo mediante la función `pl.read_excel()`, usando el parámetro `sheet_name` para especificar el nombre de la hoja de interés. Podemos, igualmente, guardar cualquier DataFrame de interés en este formato en **<font color=\"mediumorchid\">Polars</font>**, haciendo uso del método `write_excel()`, especificando la ruta y el nombre del archivo resultante mediante el parámetro `workbook`. Sin embargo, para ello, necesitamos instalar primero la librería `xlsxwriter`, a fin de dotar a **<font color=\"mediumorchid\">Polars</font>** con la capacidad de crear este tipo de archivos:\n",
    "\n",
    "    pip install XlsxWriter\n",
    "\n",
    "Y ahora sí ya estamos en condiciones de almacenar nuestro DataFrame en un formato de libro de Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a1f414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xlsxwriter.workbook.Workbook at 0x7fa3b0abdc40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Almacenamos el DataFrame en un formato de Excel (con un nombre diferente).\n",
    "data.write_excel(workbook=\"datasets/pillars_data_copy.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168883d",
   "metadata": {},
   "source": [
    "Por supuesto, existen otros formatos de archivos a los cuales podemos acceder por medio de **<font color=\"mediumorchid\">Polars</font>**. Una opción muy popular, conforme lo visto en **<font color=\"mediumorchid\">Pandas</font>**, corresponde a los archivos `csv`, los cuales pueden cargarse fácilmente por medio de la función `pl_read_csv()`, especificando la ruta y/o nombre del archivo de interés por medio del parámetro `source`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ac32a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos un archivo csv por medio de Polars.\n",
    "data = pl.read_csv(source=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d61f3012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 7)\n",
      "┌────────────┬────────────┬──────┬──────────┬────────────────┬───────────┬─────────────┐\n",
      "│ x          ┆ y          ┆ z    ┆ area     ┆ frec_fracturas ┆ sigma_z   ┆ tiraje_prom │\n",
      "│ ---        ┆ ---        ┆ ---  ┆ ---      ┆ ---            ┆ ---       ┆ ---         │\n",
      "│ f64        ┆ f64        ┆ i64  ┆ f64      ┆ f64            ┆ f64       ┆ f64         │\n",
      "╞════════════╪════════════╪══════╪══════════╪════════════════╪═══════════╪═════════════╡\n",
      "│ 776.718035 ┆ 259.185135 ┆ 1125 ┆ 286.5643 ┆ 1.979          ┆ 11.126392 ┆ 122.685     │\n",
      "│ 811.143435 ┆ 263.059235 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 9.646313  ┆ 121.178     │\n",
      "│ 845.568835 ┆ 266.933335 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 10.590594 ┆ 118.942     │\n",
      "│ 879.994235 ┆ 270.807435 ┆ 1125 ┆ 286.5643 ┆ 1.75           ┆ 10.605144 ┆ 120.012     │\n",
      "│ 914.419635 ┆ 274.681535 ┆ 1125 ┆ 286.5643 ┆ 0.67           ┆ 10.059537 ┆ 122.787     │\n",
      "└────────────┴────────────┴──────┴──────────┴────────────────┴───────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras 5 filas de este DataFrame.\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ec2aa",
   "metadata": {},
   "source": [
    "El archivo cuya data hemos cargado corresponde a una tabla que muestra algunos parámetros de interés relativos a unos pilares del nivel de producción de una mina explotada mediante el método Panel Caving. Las columnas que definen a esta tabla (y que se muestran en el DataFrame) son las siguientes:\n",
    "\n",
    "- `x`: Coordenada X (en metros) del pilar en el sistema de referencia de la mina.\n",
    "- `y`: Coordenada Y (en metros) del pilar en el sistema de referencia de la mina.\n",
    "- `z`: Coordenada Z (en metros) del pilar en el sistema de referencia de la mina.\n",
    "- `area`: Área del pilar (en metros cuadrados).\n",
    "- `frec_fracturas`: Frecuencia de fracturas asociada al pilar (en número de fracturas por metro lineal), y que corresponde a una estimación de la calidad geotécnica del macizo rocoso que constituye cada pilar.\n",
    "- `sigma_z`: Carga vertical pre-minería (en MPa) estimada sobre el pilar mediante modelamiento numérico de esfuerzos.\n",
    "- `tiraje_prom`: Tonelaje promedio de mineral extraído desde los puntos de extracción inmediatamente adyacentes a cada pilar.\n",
    "\n",
    "Como en el caso de la carga directa de datos desde archivos de Excel, **<font color=\"mediumorchid\">Polars</font>**, suele ser mucho más rápido que **<font color=\"mediumorchid\">Pandas</font>** al cargar archivos de tipo `csv`. Para el caso de nuestro archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252928e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.78 ms ± 37.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "569 µs ± 10.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pd.read_csv(filepath_or_buffer=\"datasets/pillars_data.csv\")\n",
    "%timeit pl.read_csv(source=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba47ef7",
   "metadata": {},
   "source": [
    "Vemos pues que **<font color=\"mediumorchid\">Polars</font>** carga el mismo archivo `csv`, aproximadamente, cuatro veces más rápido que **<font color=\"mediumorchid\">Pandas</font>**.\n",
    "\n",
    "Existen varios parámetros de interés que podemos manipular en la función `pl.read_csv()`. Algunos que puntualmente resultan muy importantes son:\n",
    "\n",
    "- `try_parse_dates`: Parámetro Booleano que permite dejar que **<font color=\"mediumorchid\">Polars</font>** intente especificar datos que podrían representar fechas, transformando tales datos en un formato adecuado a dichas fechas (siempre que corresponda). \n",
    "- `null_values`: Parámetro que permite especificar uno o más valores (normalmente strings) en una lista que serán intepreetados por **<font color=\"mediumorchid\">Polars</font>** como datos de tipo `nan`. Se trata de un parámetro especialmente útil cuando descargamos datos directamente desde sensores, ya que, si conocemos los códigos de error, podemos usarlos para que **<font color=\"mediumorchid\">Polars</font>** automáticamente los interprete como datos nulos y, de esa manera, el tipo de dato asociado a una determinada columna no se vea distorsionado. En **<font color=\"mediumorchid\">Pandas</font>** existe un parámetro que hace exactamente lo mismo para el caso de la función `pd.read_csv()`, llamado `na_values`.\n",
    "- `separator`: Parámetro que permite especificar el caracter que actúa como delimitador de cada columna en el archivo. En archivos de tipo `csv`, dicho separador suele ser una coma (que corresponde al parámetro por defecto, y que se especifica como `\",\"`), pero puede haber casos en los cuales el separador es un punto y coma (`\";\"`), u otros caracteres que podemos revisar en la correspondiente [documentación](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_csv.html).\n",
    "\n",
    "**<font color=\"mediumorchid\">Polars</font>** igualmente nos permite guardar cualquier serie o DataFrame en nuestro computador en formato `csv`, haciendo uso del método `write_csv()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a7fe344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado de nuestro DataFrame en formato csv.\n",
    "data.write_csv(file=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd134e0",
   "metadata": {},
   "source": [
    "**<font color=\"mediumorchid\">Polars</font>** también nos permite **escanear** cualquier archivo `csv` y guardarlo en una estructura de datos (u objeto) llamada `LazyFrame`. Tal estructura es muy parecida a un DataFrame, con la **enorme** diferencia de que un LazyFrame no dispondrá de ningún agregado visual que nos permita gastar recursos en imprimirlo, mostrarlo o *jugar* con él. Vale decir, se trata de un objeto donde *sabemos* que están nuestros datos, pero que no mostraremos (ni dejaremos que **<font color=\"mediumorchid\">Polars</font>** lo vea) a no ser que *nosotros* lo deseemos.\n",
    "\n",
    "El escaneo de un archivo `csv` puede realizarse por medio de la función `pl_scan_csv()`, y que tiene casi los mismos parámetros que `pl.read_csv()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ffb4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_csv(source=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa973407",
   "metadata": {},
   "source": [
    "La variable `df` es, en efecto, un LazyFrame de **<font color=\"mediumorchid\">Polars</font>**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e53ec5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.lazyframe.frame.LazyFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0f197",
   "metadata": {},
   "source": [
    "Estos objetos son propios de la llamada **lazy API** de **<font color=\"mediumorchid\">Polars</font>**, y que corresponde a una interfaz extremadamente eficiente con el gasto de recursos computacionales a la hora de manipular estructuras de datos. Tal eficiencia llegar al punto de ni siquiera gastar recursos en mostrar los datos almacenados en pantalla al intentar imprimirlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3592ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive plan: (run LazyFrame.explain(optimized=True) to see the optimized plan)\n",
      "\n",
      "\n",
      "  CSV SCAN datasets/pillars_data.csv\n",
      "  PROJECT */7 COLUMNS\n"
     ]
    }
   ],
   "source": [
    "# Un LazyFrame no se imprime en pantalla.\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b827a54",
   "metadata": {},
   "source": [
    "Si queremos, podemos extraer los datos de un LazyFrame por medio del método `fetch()`, usando el parámetro `n_rows` para especificar cuántas filas queremos recuperar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec9a8648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 7)\n",
      "┌────────────┬────────────┬──────┬──────────┬────────────────┬───────────┬─────────────┐\n",
      "│ x          ┆ y          ┆ z    ┆ area     ┆ frec_fracturas ┆ sigma_z   ┆ tiraje_prom │\n",
      "│ ---        ┆ ---        ┆ ---  ┆ ---      ┆ ---            ┆ ---       ┆ ---         │\n",
      "│ f64        ┆ f64        ┆ i64  ┆ f64      ┆ f64            ┆ f64       ┆ f64         │\n",
      "╞════════════╪════════════╪══════╪══════════╪════════════════╪═══════════╪═════════════╡\n",
      "│ 776.718035 ┆ 259.185135 ┆ 1125 ┆ 286.5643 ┆ 1.979          ┆ 11.126392 ┆ 122.685     │\n",
      "│ 811.143435 ┆ 263.059235 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 9.646313  ┆ 121.178     │\n",
      "│ 845.568835 ┆ 266.933335 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 10.590594 ┆ 118.942     │\n",
      "│ 879.994235 ┆ 270.807435 ┆ 1125 ┆ 286.5643 ┆ 1.75           ┆ 10.605144 ┆ 120.012     │\n",
      "│ 914.419635 ┆ 274.681535 ┆ 1125 ┆ 286.5643 ┆ 0.67           ┆ 10.059537 ┆ 122.787     │\n",
      "└────────────┴────────────┴──────┴──────────┴────────────────┴───────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Recuperamos las 5 primeras filas de nuestro LazyFrame.\n",
    "print(df.fetch(n_rows=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6330e",
   "metadata": {},
   "source": [
    "El acceso a los datos de un archivo `csv` por medio de un escaneo permite igualmente ahorrar tiempo de ejecución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d57ea369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560 µs ± 7.67 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "187 µs ± 2.13 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pl.read_csv(source=\"datasets/pillars_data.csv\")\n",
    "%timeit pl.scan_csv(source=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b652325",
   "metadata": {},
   "source": [
    "Vemos que `pl.scan_csv()` se ejecuta aproximadamente tres veces más rápido que `pl.read_csv()`.\n",
    "\n",
    "**<font color=\"mediumorchid\">Polars</font>** nos permite cargar datos desde otros tipos de archivos, incluyendo JSON, bases de datos y parquet. Por el momento, nos limitaremos a archivos estáticos propios de máquinas locales como `xlsx` y `csv`, y dejaremos el resto para un momento posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca520630",
   "metadata": {},
   "source": [
    "## Iteraciones sobre filas... otra vez.\n",
    "Como en el caso de **<font color=\"mediumorchid\">Pandas</font>**, no es recomendable la iteración por filas en **<font color=\"mediumorchid\">Polars</font>**, ya que no resulta eficiente en términos de tiempos de ejecución. Sin embargo, ello no quiere decir que **<font color=\"mediumorchid\">Polars</font>** no nos ofrezca formad de hacerlo en caso de ser necesario. El método apto para construir iteraciones de este tipo, aplicable sobre series y DataFrames, es `iter_rows()`.\n",
    "\n",
    "`iter_rows()` permite retornar filas de un DataFrame en un formato de tuplas o de diccionarios, de manera similar al método `row()`, dependiendo de si usamos el parámetro Booleano `named`. Sin embargo, la diferencia esencial entre ambos métodos, es que `iter_rows()` nos permite construir **generadores** aptos para cualquier tipo de cálculo iterativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "443298b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataFrame.iter_rows at 0x7fa3b207e9e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El método iter_rows() nos permite construir generadores.\n",
    "data.iter_rows(named=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64649260",
   "metadata": {},
   "source": [
    "El generador anterior nos permite construir cálculos iterativos de cualquier índole. Por ejemplo, mediante comprensiones de listas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "835f3439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.126392, 9.646313, 10.590594, 10.605144, 10.059537]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construimos una lista con los primeros 5 valores del esfuerzo vertical pre-minería\n",
    "# (sigma_z) para cada pilar.\n",
    "[row[\"sigma_z\"] for row in data.head().iter_rows(named=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca148270",
   "metadata": {},
   "source": [
    "Y, por supuesto, también mediante bucles completos. Para ejemplificar una iteración por filas usando un bucle de tipo `for`, vamos a construir una nueva columna llamada `calidad_roca`, y que categorizará la calidad geotécnica de cada pilar conforme la siguiente clasificación:\n",
    "\n",
    "- Si $FF\\leq 1.25$, entonces la roca es de buena calidad.\n",
    "- Si $FF\\in (1.25,2.25]$, entonces la roca es de calidad regular.\n",
    "- Si $FF>2.25$, entonces la roca es de mala calidad.\n",
    "\n",
    "Lo que haremos será crear esta columna usando el método `iter_rows()`, asignando una categoría correspondiente dependiendo del valor asociado a la columna `frec_fracturas` en cada una de las filas del DataFrame, asignando tales valores a una serie previamente construida, con un tipo de dato conveniente, asignando luego dicha serie como una nueva columna al DataFrame mediante el método `with_columns()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d32a4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos una serie con valores de tipo string iguales a \"a\".\n",
    "s = pl.Series(values=np.full(fill_value=\"a\", shape=data.shape[0]), name=\"calidad_roca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5527ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediante una iteración por filas, determinamos la categoría asociada a la calidad de\n",
    "# la roca de los pilares y asignamos cada valor a la serie anterior.\n",
    "for i, row_i in enumerate(data.iter_rows(named=True)):\n",
    "    if row_i[\"frec_fracturas\"] <= 1.25:\n",
    "        qa_i = \"Buena calidad\"\n",
    "    elif  row_i[\"frec_fracturas\"] > 2.25:\n",
    "        qa_i = \"Mala calidad\"\n",
    "    else:\n",
    "        qa_i = \"Calidad regular\"\n",
    "    s[i] = qa_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d4c04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos la serie anterior a nuestro DataFrame.\n",
    "data = data.with_columns([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a54adc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 8)\n",
      "┌────────────┬────────────┬──────┬──────────┬────────────┬───────────┬─────────────┬───────────────┐\n",
      "│ x          ┆ y          ┆ z    ┆ area     ┆ frec_fract ┆ sigma_z   ┆ tiraje_prom ┆ calidad_roca  │\n",
      "│ ---        ┆ ---        ┆ ---  ┆ ---      ┆ uras       ┆ ---       ┆ ---         ┆ ---           │\n",
      "│ f64        ┆ f64        ┆ i64  ┆ f64      ┆ ---        ┆ f64       ┆ f64         ┆ str           │\n",
      "│            ┆            ┆      ┆          ┆ f64        ┆           ┆             ┆               │\n",
      "╞════════════╪════════════╪══════╪══════════╪════════════╪═══════════╪═════════════╪═══════════════╡\n",
      "│ 776.718035 ┆ 259.185135 ┆ 1125 ┆ 286.5643 ┆ 1.979      ┆ 11.126392 ┆ 122.685     ┆ Calidad       │\n",
      "│            ┆            ┆      ┆          ┆            ┆           ┆             ┆ regular       │\n",
      "│ 811.143435 ┆ 263.059235 ┆ 1125 ┆ 286.5643 ┆ 3.07       ┆ 9.646313  ┆ 121.178     ┆ Mala calidad  │\n",
      "│ 845.568835 ┆ 266.933335 ┆ 1125 ┆ 286.5643 ┆ 3.07       ┆ 10.590594 ┆ 118.942     ┆ Mala calidad  │\n",
      "│ 879.994235 ┆ 270.807435 ┆ 1125 ┆ 286.5643 ┆ 1.75       ┆ 10.605144 ┆ 120.012     ┆ Calidad       │\n",
      "│            ┆            ┆      ┆          ┆            ┆           ┆             ┆ regular       │\n",
      "│ 914.419635 ┆ 274.681535 ┆ 1125 ┆ 286.5643 ┆ 0.67       ┆ 10.059537 ┆ 122.787     ┆ Buena calidad │\n",
      "└────────────┴────────────┴──────┴──────────┴────────────┴───────────┴─────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas de nuestro DataFrame.\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136506a2",
   "metadata": {},
   "source": [
    "Vamos a replicar todo el proceso anterior por medio de una función, a fin de poder medir el tiempo de ejecución de este procedimiento. Haremos un procedimiento similar en **<font color=\"mediumorchid\">Pandas</font>**, de manera tal que podamos comparar los tiempos de ejecución resultantes en ambas librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "548b4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metemos todo el proceso anterior vía Polars en una función.\n",
    "def classify_rock_qa_polars(data: pl.DataFrame) -> pd.DataFrame:\n",
    "    # Inicializamos una serie con valores de tipo string iguales a \"a\".\n",
    "    s = pl.Series(values=np.full(fill_value=\"a\", shape=data.shape[0]), name=\"calidad_roca\")\n",
    "    \n",
    "    # Mediante una iteración por filas, determinamos la categoría asociada a la calidad de\n",
    "    # la roca de los pilares y asignamos cada valor a la serie anterior.\n",
    "    for i, row_i in enumerate(data.iter_rows(named=True)):\n",
    "        if row_i[\"frec_fracturas\"] <= 1.25:\n",
    "            qa_i = \"Buena calidad\"\n",
    "        elif  row_i[\"frec_fracturas\"] > 2.25:\n",
    "            qa_i = \"Mala calidad\"\n",
    "        else:\n",
    "            qa_i = \"Calidad regular\"\n",
    "        s[i] = qa_i\n",
    "    \n",
    "    # Asignamos la serie anterior a nuestro DataFrame.\n",
    "    data = data.with_columns([s])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fe02f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y también metemos todo en un proceso de Pandas, a fin de comparar.\n",
    "# Una función para iterar por filas en un DataFrame de Pandas y hacer la clasificación.\n",
    "def classify_rock_qa_pandas(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Definimos la nueva columna y la inicializamos con un NaN.\n",
    "    data[\"calidad_roca\"] = np.nan\n",
    "\n",
    "    # Clasificamos los valores de la frecuencia de fracturas mediante un loop.\n",
    "    for row in data.index:\n",
    "        if data.loc[row, \"frec_fracturas\"] <= 1.25:\n",
    "            data.loc[row, \"calidad_roca\"] = \"Buena calidad\"\n",
    "        elif data.loc[row, \"frec_fracturas\"] > 2.25:\n",
    "            data.loc[row, \"calidad_roca\"] = \"Mala calidad\"\n",
    "        else:\n",
    "            data.loc[row, \"calidad_roca\"] = \"Calidad regular\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b396bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos en un DataFrame de Pandas.\n",
    "df = pd.read_csv(filepath_or_buffer=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49b27cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 ms ± 4.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "154 ms ± 2.36 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit classify_rock_qa_polars(data)\n",
    "%timeit classify_rock_qa_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6569a02",
   "metadata": {},
   "source": [
    "Vemos pues que, para este caso particular, **<font color=\"mediumorchid\">Polars</font>** es igual de ineficiente que **<font color=\"mediumorchid\">Pandas</font>** a la hora de generar iteraciones por filas sobre DataFrames. Y, además, su sintaxis no resulta en absoluto familiar con respecto a la de **<font color=\"mediumorchid\">Pandas</font>** o, incluso, de **<font color=\"mediumorchid\">Numpy</font>**. Tomaremos ésto como una motivación adicional para hacer todo el esfuerzo posible a fin de vectorizar las operaciones que queramos realizar sobre estructuras de datos de **<font color=\"mediumorchid\">Polars</font>**. La anterior, puntualmente, podemos realizarla fácilmente mediante la función `np.where()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9f9fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos nuevamente nuestra data en un DataFrame de Polars, a fin de partir desde cero.\n",
    "data = pl.read_csv(source=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7da4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una forma más eficiente de realizar la operación anterior.\n",
    "def classify_rock_qa_vect(data: pl.DataFrame) -> pl.DataFrame:\n",
    "    s = np.where(\n",
    "        data[\"frec_fracturas\"] <= 1.25, \"Buena calidad\",\n",
    "        np.where(data[\"frec_fracturas\"] > 2.25, \"Mala calidad\", \"Calidad regular\")\n",
    "    )\n",
    "    s = pl.Series(values=s)\n",
    "    df = data.with_columns([s.alias(\"calidad_roca\")])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d08f32d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747 µs ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Revisamos su tiempo de ejecución.\n",
    "%timeit classify_rock_qa_vect(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1801571",
   "metadata": {},
   "source": [
    "¡Esta función es más de 200 veces más rápida que la anterior, tras haber evitado la iteración por filas!\n",
    "\n",
    "Y ahora sí, ya podemos crear nuestra columna que permite categorizar la calidad de la roca constituyente de nuestros pilares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7735324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos nuestra función vectorizada.\n",
    "data = classify_rock_qa_vect(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cb37ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 8)\n",
      "┌────────────┬────────────┬──────┬──────────┬────────────┬───────────┬─────────────┬───────────────┐\n",
      "│ x          ┆ y          ┆ z    ┆ area     ┆ frec_fract ┆ sigma_z   ┆ tiraje_prom ┆ calidad_roca  │\n",
      "│ ---        ┆ ---        ┆ ---  ┆ ---      ┆ uras       ┆ ---       ┆ ---         ┆ ---           │\n",
      "│ f64        ┆ f64        ┆ i64  ┆ f64      ┆ ---        ┆ f64       ┆ f64         ┆ str           │\n",
      "│            ┆            ┆      ┆          ┆ f64        ┆           ┆             ┆               │\n",
      "╞════════════╪════════════╪══════╪══════════╪════════════╪═══════════╪═════════════╪═══════════════╡\n",
      "│ 776.718035 ┆ 259.185135 ┆ 1125 ┆ 286.5643 ┆ 1.979      ┆ 11.126392 ┆ 122.685     ┆ Calidad       │\n",
      "│            ┆            ┆      ┆          ┆            ┆           ┆             ┆ regular       │\n",
      "│ 811.143435 ┆ 263.059235 ┆ 1125 ┆ 286.5643 ┆ 3.07       ┆ 9.646313  ┆ 121.178     ┆ Mala calidad  │\n",
      "│ 845.568835 ┆ 266.933335 ┆ 1125 ┆ 286.5643 ┆ 3.07       ┆ 10.590594 ┆ 118.942     ┆ Mala calidad  │\n",
      "│ 879.994235 ┆ 270.807435 ┆ 1125 ┆ 286.5643 ┆ 1.75       ┆ 10.605144 ┆ 120.012     ┆ Calidad       │\n",
      "│            ┆            ┆      ┆          ┆            ┆           ┆             ┆ regular       │\n",
      "│ 914.419635 ┆ 274.681535 ┆ 1125 ┆ 286.5643 ┆ 0.67       ┆ 10.059537 ┆ 122.787     ┆ Buena calidad │\n",
      "└────────────┴────────────┴──────┴──────────┴────────────┴───────────┴─────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas de nuestro DataFrame.\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b0574",
   "metadata": {},
   "source": [
    "## Agregaciones.\n",
    "\n",
    "Cuando empezamos a dar nuestros primeros pasos en **<font color=\"mediumorchid\">Polars</font>**, aprendimos que sus estructuras de datos pueden manipularse por medio de dos conjuntos importantes de transformaciones propias de su lenguaje de dominio específico (DSL), denominadas **contextos** y **expresiones**. Dos de los contextos ya fueron revisados en detalle previamente, siendo usados fundamentalmente para seleccionar datos (mediante los métodos `select_columns()` y  `with_columns()`) y filtrarlos (por medio del método `filter()`). No obstante, nos quedó un contexto pendiente y que nos comprometimos a revisar, y que corresponde a la construcción de **agregaciones** por medio de los métodos `groupby()` y `agg()`.\n",
    "\n",
    "Bajo el contexto `groupby()`, podemos trabajar cualquier tipo de expresión de **<font color=\"mediumorchid\">Polars</font>** por medio de grupos construidos a partir de categorías existentes en un DataFrame, de tal forma que podamos generar cualquier tipo de agregación a partir de tales grupos. Por ejemplo, en nuestro DataFrame anterior (`data`), podemos usar el contexto `groupby()` para construir cualquier tipo de agregación en función de las categorías previamente construidas para calificar la calidad del macizo rocoso constituyente de cada pilar. La agregación toma como argumento el nombre de la columna que define a los grupos (en nuestro caso, `\"calidad_roca\"`), y luego hace uso del método `agg()` para generar las agregaciones que queramos, debido a que el resultado de la aplicación del método `groupby()` es un objeto preparado para construir tales agregaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eddfb954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<polars.dataframe.groupby.GroupBy at 0x7fa3b242e850>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"calidad_roca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d429f9",
   "metadata": {},
   "source": [
    "De esta manera, construiremos la siguiente agregación: Calcularemos el promedio de la carga vertical pre-minería (columna `\"sigma_z\"`) y de la frecuencia de fracturas por metro lineal (columna `\"frec_fracturas\"`), y el máximo valor de tiraje adyacente (columna `\"tiraje_prom\"`) asociados a cada una de las calidades de macizo rocoso previamente definidas. Cada una de estas agregaciones se mostrará con un determinado alias, que definimos en la misma agregación por medio del método `alias()`, de la misma forma en que hicimos con los contextos `select_columns()`, `with_columns()` y `filter()`. La preservación de la **sintaxis contextual** de **<font color=\"mediumorchid\">Polars</font>** es, por supuesto, parte de su lenguaje de dominio específico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3fb2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos las agregaciones encadenando al contexto groupby() el método agg().\n",
    "result = data.groupby(\"calidad_roca\").agg(\n",
    "    pl.col(\"sigma_z\").mean().alias(\"avg_sigma_z\"),\n",
    "    pl.col(\"frec_fracturas\").mean().alias(\"avg_frec_fracturas\"),\n",
    "    pl.col(\"tiraje_prom\").max().alias(\"max_tiraje_ady\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de1af1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 4)\n",
      "┌─────────────────┬─────────────┬────────────────────┬────────────────┐\n",
      "│ calidad_roca    ┆ avg_sigma_z ┆ avg_frec_fracturas ┆ max_tiraje_ady │\n",
      "│ ---             ┆ ---         ┆ ---                ┆ ---            │\n",
      "│ str             ┆ f64         ┆ f64                ┆ f64            │\n",
      "╞═════════════════╪═════════════╪════════════════════╪════════════════╡\n",
      "│ Calidad regular ┆ 12.355339   ┆ 1.751776           ┆ 193.471        │\n",
      "│ Mala calidad    ┆ 12.192455   ┆ 3.003736           ┆ 192.489        │\n",
      "│ Buena calidad   ┆ 13.552295   ┆ 0.735853           ┆ 201.307        │\n",
      "└─────────────────┴─────────────┴────────────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado.\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
