{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ae212e",
   "metadata": {},
   "source": [
    "# CLASE 5.2: MANIPULACIÓN DE DATOS EN ESTRUCTURAS DE **<font color=\"mediumorchid\">POLARS</font>**.\n",
    "---\n",
    "\n",
    "## Carga y guardado de datos en **<font color=\"mediumorchid\">Polars</font>**.\n",
    "Como ya hemos visto en todo el desarrollo de la asignatura, en general, el acceso a la información de interés propia de algún fenómeno, proceso o sistema que estamos interesados en analizar, suele provenir de archivos externos, muchas veces preprocesados por terceros. Por lo tanto, es común que hagamos uso de funciones especializadas en leer y/o acceder a tales archivos, o bien, que guarden las series y/o DataFrames que construyamos como resultado de algún análisis en nuestro computador. En **<font color=\"mediumorchid\">Polars</font>**, tales funciones se resumen en un conjunto conocido como **IO** (del inglés **Input/Output**).\n",
    "\n",
    "Antes de comentar las funciones de IO propias de **<font color=\"mediumorchid\">Polars</font>**, importaremos esta librería (junto con **<font color=\"mediumorchid\">Numpy</font>** y **<font color=\"mediumorchid\">Pandas</font>**) a fin de hacer las correspondientes comparaciones en tiempo de ejecución que, en esta sección, sí serán de enorme importancia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54276979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e84fb1",
   "metadata": {},
   "source": [
    "En esta sección, haremos uso de algunos archivos ubicados en la carpeta `datasets`, a fin de ejemplificar en primera instancia el cómo acceder a la data contenida en los mismos mediante el uso de algunas funciones de IO en **<font color=\"mediumorchid\">Polars</font>**. Partiremos, como en el caso de **<font color=\"mediumorchid\">Pandas</font>**, mostrando el acceso a archivos nativos de Microsoft Excel, a los que podremos acceder siempre por medio de la función `pl.read_excel()`. Sin embargo, para ello, será necesario instalar un motor de lectura de archivos de este tipo distinto del usado en **<font color=\"mediumorchid\">Pandas</font>**, llamado `xlsx2csv`. Para ello, mediante nuestra consola, escribimos la instrucción:\n",
    "\n",
    "    pip install xlsx2csv\n",
    "\n",
    "Y ya podemos acceder a nuestros archivos de Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5038ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceso a un archivo nativo de Excel en Polars.\n",
    "data = pl.read_excel(source=\"datasets/pillars_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de33cabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 7)\n",
      "┌────────────┬────────────┬──────┬──────────┬────────────────┬───────────┬─────────────┐\n",
      "│ x          ┆ y          ┆ z    ┆ area     ┆ frec_fracturas ┆ sigma_z   ┆ tiraje_prom │\n",
      "│ ---        ┆ ---        ┆ ---  ┆ ---      ┆ ---            ┆ ---       ┆ ---         │\n",
      "│ f64        ┆ f64        ┆ i64  ┆ f64      ┆ f64            ┆ f64       ┆ f64         │\n",
      "╞════════════╪════════════╪══════╪══════════╪════════════════╪═══════════╪═════════════╡\n",
      "│ 776.718035 ┆ 259.185135 ┆ 1125 ┆ 286.5643 ┆ 1.979          ┆ 11.126392 ┆ 122.685     │\n",
      "│ 811.143435 ┆ 263.059235 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 9.646313  ┆ 121.178     │\n",
      "│ 845.568835 ┆ 266.933335 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 10.590594 ┆ 118.942     │\n",
      "│ 879.994235 ┆ 270.807435 ┆ 1125 ┆ 286.5643 ┆ 1.75           ┆ 10.605144 ┆ 120.012     │\n",
      "│ 914.419635 ┆ 274.681535 ┆ 1125 ┆ 286.5643 ┆ 0.67           ┆ 10.059537 ┆ 122.787     │\n",
      "└────────────┴────────────┴──────┴──────────┴────────────────┴───────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras 5 filas de este DataFrame.\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc898b6",
   "metadata": {},
   "source": [
    "Como en el caso de **<font color=\"mediumorchid\">Pandas</font>**, el acceso a archivos nativos y/o creados por Microsoft Excel en **<font color=\"mediumorchid\">Polars</font>** es una tarea que consume varios recursos de memoria, y puede catalogarse como de ejecución *lenta*. Sin embargo, **<font color=\"mediumorchid\">Polars</font>** es capaz de acceder a estos archivos más rápido que **<font color=\"mediumorchid\">Pandas</font>**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e0a35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.6 ms ± 4.16 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "33.5 ms ± 1.78 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pd.read_excel(io=\"datasets/pillars_data.xlsx\")\n",
    "%timeit pl.read_excel(source=\"datasets/pillars_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a33da",
   "metadata": {},
   "source": [
    "Es posible seleccionar cualquier hoja en un libro de Excel accediendo al archivo mediante la función `pl.read_excel()`, usando el parámetro `sheet_name` para especificar el nombre de la hoja de interés. Podemos, igualmente, guardar cualquier DataFrame de interés en este formato en **<font color=\"mediumorchid\">Polars</font>**, haciendo uso del método `write_excel()`, especificando la ruta y el nombre del archivo resultante mediante el parámetro `workbook`. Sin embargo, para ello, necesitamos instalar primero la librería `xlsxwriter`, a fin de dotar a **<font color=\"mediumorchid\">Polars</font>** con la capacidad de crear este tipo de archivos:\n",
    "\n",
    "    pip install XlsxWriter\n",
    "\n",
    "Y ahora sí ya estamos en condiciones de almacenar nuestro DataFrame en un formato de libro de Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9061bdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xlsxwriter.workbook.Workbook at 0x7fd799b38e50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Almacenamos el DataFrame en un formato de Excel (con un nombre diferente).\n",
    "data.write_excel(workbook=\"datasets/pillars_data_copy.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c7cae",
   "metadata": {},
   "source": [
    "Por supuesto, existen otros formatos de archivos a los cuales podemos acceder por medio de **<font color=\"mediumorchid\">Polars</font>**. Una opción muy popular, conforme lo visto en **<font color=\"mediumorchid\">Pandas</font>**, corresponde a los archivos `csv`, los cuales pueden cargarse fácilmente por medio de la función `pl_read_csv()`, especificando la ruta y/o nombre del archivo de interés por medio del parámetro `source`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05367b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos un archivo csv por medio de Polars.\n",
    "data = pl.read_csv(source=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8418265e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 7)\n",
      "┌────────────┬────────────┬──────┬──────────┬────────────────┬───────────┬─────────────┐\n",
      "│ x          ┆ y          ┆ z    ┆ area     ┆ frec_fracturas ┆ sigma_z   ┆ tiraje_prom │\n",
      "│ ---        ┆ ---        ┆ ---  ┆ ---      ┆ ---            ┆ ---       ┆ ---         │\n",
      "│ f64        ┆ f64        ┆ i64  ┆ f64      ┆ f64            ┆ f64       ┆ f64         │\n",
      "╞════════════╪════════════╪══════╪══════════╪════════════════╪═══════════╪═════════════╡\n",
      "│ 776.718035 ┆ 259.185135 ┆ 1125 ┆ 286.5643 ┆ 1.979          ┆ 11.126392 ┆ 122.685     │\n",
      "│ 811.143435 ┆ 263.059235 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 9.646313  ┆ 121.178     │\n",
      "│ 845.568835 ┆ 266.933335 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 10.590594 ┆ 118.942     │\n",
      "│ 879.994235 ┆ 270.807435 ┆ 1125 ┆ 286.5643 ┆ 1.75           ┆ 10.605144 ┆ 120.012     │\n",
      "│ 914.419635 ┆ 274.681535 ┆ 1125 ┆ 286.5643 ┆ 0.67           ┆ 10.059537 ┆ 122.787     │\n",
      "└────────────┴────────────┴──────┴──────────┴────────────────┴───────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras 5 filas de este DataFrame.\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87e335",
   "metadata": {},
   "source": [
    "El archivo cuya data hemos cargado corresponde a una tabla que muestra algunos parámetros de interés relativos a unos pilares del nivel de producción de una mina explotada mediante el método Panel Caving. Las columnas que definen a esta tabla (y que se muestran en el DataFrame) son las siguientes:\n",
    "\n",
    "- `x`: Coordenada X (en metros) del pilar en el sistema de referencia de la mina.\n",
    "- `y`: Coordenada Y (en metros) del pilar en el sistema de referencia de la mina.\n",
    "- `z`: Coordenada Z (en metros) del pilar en el sistema de referencia de la mina.\n",
    "- `area`: Área del pilar (en metros cuadrados).\n",
    "- `frec_fracturas`: Frecuencia de fracturas asociada al pilar (en número de fracturas por metro lineal), y que corresponde a una estimación de la calidad geotécnica del macizo rocoso que constituye cada pilar.\n",
    "- `sigma_z`: Carga vertical pre-minería (en MPa) estimada sobre el pilar mediante modelamiento numérico de esfuerzos.\n",
    "- `tiraje_prom`: Tonelaje promedio de mineral extraído desde los puntos de extracción inmediatamente adyacentes a cada pilar.\n",
    "\n",
    "Como en el caso de la carga directa de datos desde archivos de Excel, **<font color=\"mediumorchid\">Polars</font>**, suele ser mucho más rápido que **<font color=\"mediumorchid\">Pandas</font>** al cargar archivos de tipo `csv`. Para el caso de nuestro archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d5ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6 ms ± 84.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "327 µs ± 4.08 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pd.read_csv(filepath_or_buffer=\"datasets/pillars_data.csv\")\n",
    "%timeit pl.read_csv(source=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1d80d",
   "metadata": {},
   "source": [
    "Vemos pues que **<font color=\"mediumorchid\">Polars</font>** carga el mismo archivo `csv`, aproximadamente, cuatro veces más rápido que **<font color=\"mediumorchid\">Pandas</font>**.\n",
    "\n",
    "Existen varios parámetros de interés que podemos manipular en la función `pl.read_csv()`. Algunos que puntualmente resultan muy importantes son:\n",
    "\n",
    "- `try_parse_dates`: Parámetro Booleano que permite dejar que **<font color=\"mediumorchid\">Polars</font>** intente especificar datos que podrían representar fechas, transformando tales datos en un formato adecuado a dichas fechas (siempre que corresponda). \n",
    "- `null_values`: Parámetro que permite especificar uno o más valores (normalmente strings) en una lista que serán intepreetados por **<font color=\"mediumorchid\">Polars</font>** como datos de tipo `nan`. Se trata de un parámetro especialmente útil cuando descargamos datos directamente desde sensores, ya que, si conocemos los códigos de error, podemos usarlos para que **<font color=\"mediumorchid\">Polars</font>** automáticamente los interprete como datos nulos y, de esa manera, el tipo de dato asociado a una determinada columna no se vea distorsionado. En **<font color=\"mediumorchid\">Pandas</font>** existe un parámetro que hace exactamente lo mismo para el caso de la función `pd.read_csv()`, llamado `na_values`.\n",
    "- `separator`: Parámetro que permite especificar el caracter que actúa como delimitador de cada columna en el archivo. En archivos de tipo `csv`, dicho separador suele ser una coma (que corresponde al parámetro por defecto, y que se especifica como `\",\"`), pero puede haber casos en los cuales el separador es un punto y coma (`\";\"`), u otros caracteres que podemos revisar en la correspondiente [documentación](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_csv.html).\n",
    "\n",
    "**<font color=\"mediumorchid\">Polars</font>** igualmente nos permite guardar cualquier serie o DataFrame en nuestro computador en formato `csv`, haciendo uso del método `write_csv()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd9d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado de nuestro DataFrame en formato csv.\n",
    "data.write_csv(file=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618f6c3",
   "metadata": {},
   "source": [
    "**<font color=\"mediumorchid\">Polars</font>** también nos permite **escanear** cualquier archivo `csv` y guardarlo en una estructura de datos (u objeto) llamada `LazyFrame`. Tal estructura es muy parecida a un DataFrame, con la **enorme** diferencia de que un LazyFrame no dispondrá de ningún agregado visual que nos permita gastar recursos en imprimirlo, mostrarlo o *jugar* con él. Vale decir, se trata de un objeto donde *sabemos* que están nuestros datos, pero que no mostraremos (ni dejaremos que **<font color=\"mediumorchid\">Polars</font>** lo vea) a no ser que *nosotros* lo deseemos.\n",
    "\n",
    "El escaneo de un archivo `csv` puede realizarse por medio de la función `pl_scan_csv()`, y que tiene casi los mismos parámetros que `pl.read_csv()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff947f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_csv(source=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338662dc",
   "metadata": {},
   "source": [
    "La variable `df` es, en efecto, un LazyFrame de **<font color=\"mediumorchid\">Polars</font>**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29344be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.lazyframe.frame.LazyFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55733d",
   "metadata": {},
   "source": [
    "Estos objetos son propios de la llamada **lazy API** de **<font color=\"mediumorchid\">Polars</font>**, y que corresponde a una interfaz extremadamente eficiente con el gasto de recursos computacionales a la hora de manipular estructuras de datos. Tal eficiencia llegar al punto de ni siquiera gastar recursos en mostrar los datos almacenados en pantalla al intentar imprimirlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3872935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive plan: (run LazyFrame.explain(optimized=True) to see the optimized plan)\n",
      "\n",
      "\n",
      "  CSV SCAN datasets/pillars_data.csv\n",
      "  PROJECT */7 COLUMNS\n"
     ]
    }
   ],
   "source": [
    "# Un LazyFrame no se imprime en pantalla.\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa1103",
   "metadata": {},
   "source": [
    "Si queremos, podemos extraer los datos de un LazyFrame por medio del método `fetch()`, usando el parámetro `n_rows` para especificar cuántas filas queremos recuperar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "629fbd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 7)\n",
      "┌────────────┬────────────┬──────┬──────────┬────────────────┬───────────┬─────────────┐\n",
      "│ x          ┆ y          ┆ z    ┆ area     ┆ frec_fracturas ┆ sigma_z   ┆ tiraje_prom │\n",
      "│ ---        ┆ ---        ┆ ---  ┆ ---      ┆ ---            ┆ ---       ┆ ---         │\n",
      "│ f64        ┆ f64        ┆ i64  ┆ f64      ┆ f64            ┆ f64       ┆ f64         │\n",
      "╞════════════╪════════════╪══════╪══════════╪════════════════╪═══════════╪═════════════╡\n",
      "│ 776.718035 ┆ 259.185135 ┆ 1125 ┆ 286.5643 ┆ 1.979          ┆ 11.126392 ┆ 122.685     │\n",
      "│ 811.143435 ┆ 263.059235 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 9.646313  ┆ 121.178     │\n",
      "│ 845.568835 ┆ 266.933335 ┆ 1125 ┆ 286.5643 ┆ 3.07           ┆ 10.590594 ┆ 118.942     │\n",
      "│ 879.994235 ┆ 270.807435 ┆ 1125 ┆ 286.5643 ┆ 1.75           ┆ 10.605144 ┆ 120.012     │\n",
      "│ 914.419635 ┆ 274.681535 ┆ 1125 ┆ 286.5643 ┆ 0.67           ┆ 10.059537 ┆ 122.787     │\n",
      "└────────────┴────────────┴──────┴──────────┴────────────────┴───────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Recuperamos las 5 primeras filas de nuestro LazyFrame.\n",
    "print(df.fetch(n_rows=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fbfb8",
   "metadata": {},
   "source": [
    "El acceso a los datos de un archivo `csv` por medio de un escaneo permite igualmente ahorrar tiempo de ejecución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9325d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348 µs ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "107 µs ± 1.24 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pl.read_csv(source=\"datasets/pillars_data.csv\")\n",
    "%timeit pl.scan_csv(source=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b76bd",
   "metadata": {},
   "source": [
    "Vemos que `pl.scan_csv()` se ejecuta aproximadamente tres veces más rápido que `pl.read_csv()`.\n",
    "\n",
    "**<font color=\"mediumorchid\">Polars</font>** nos permite cargar datos desde otros tipos de archivos, incluyendo JSON, bases de datos y parquet. Por el momento, nos limitaremos a archivos estáticos propios de máquinas locales como `xlsx` y `csv`, y dejaremos el resto para un momento posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5398d9",
   "metadata": {},
   "source": [
    "## Iteraciones sobre filas... otra vez.\n",
    "Como en el caso de **<font color=\"mediumorchid\">Pandas</font>**, no es recomendable la iteración por filas en **<font color=\"mediumorchid\">Polars</font>**, ya que no resulta eficiente en términos de tiempos de ejecución. Sin embargo, ello no quiere decir que **<font color=\"mediumorchid\">Polars</font>** no nos ofrezca formad de hacerlo en caso de ser necesario. El método apto para construir iteraciones de este tipo, aplicable sobre series y DataFrames, es `iter_rows()`.\n",
    "\n",
    "`iter_rows()` permite retornar filas de un DataFrame en un formato de tuplas o de diccionarios, de manera similar al método `row()`, dependiendo de si usamos el parámetro Booleano `named`. Sin embargo, la diferencia esencial entre ambos métodos, es que `iter_rows()` nos permite construir **generadores** aptos para cualquier tipo de cálculo iterativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b896d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataFrame.iter_rows at 0x7fd79a448f90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El método iter_rows() nos permite construir generadores.\n",
    "data.iter_rows(named=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655669e6",
   "metadata": {},
   "source": [
    "El generador anterior nos permite construir cálculos iterativos de cualquier índole. Por ejemplo, mediante comprensiones de listas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af559eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.126392, 9.646313, 10.590594, 10.605144, 10.059537]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construimos una lista con los primeros 5 valores del esfuerzo vertical pre-minería\n",
    "# (sigma_z) para cada pilar.\n",
    "[row[\"sigma_z\"] for row in data.head().iter_rows(named=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2573639",
   "metadata": {},
   "source": [
    "Y, por supuesto, también mediante bucles completos. Para ejemplificar una iteración por filas usando un bucle de tipo `for`, vamos a construir una nueva columna llamada `calidad_roca`, y que categorizará la calidad geotécnica de cada pilar conforme la siguiente clasificación:\n",
    "\n",
    "- Si $FF\\leq 1.25$, entonces la roca es de buena calidad.\n",
    "- Si $FF\\in (1.25,2.25]$, entonces la roca es de calidad regular.\n",
    "- Si $FF>2.25$, entonces la roca es de mala calidad.\n",
    "\n",
    "Lo que haremos será crear esta columna usando el método `iter_rows()`, asignando una categoría correspondiente dependiendo del valor asociado a la columna `frec_fracturas` en cada una de las filas del DataFrame, asignando tales valores a una serie previamente construida, con un tipo de dato conveniente, asignando luego dicha serie como una nueva columna al DataFrame mediante el método `with_columns()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4660423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos una serie con valores de tipo string iguales a \"a\".\n",
    "s = pl.Series(values=np.full(fill_value=\"a\", shape=data.shape[0]), name=\"calidad_roca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a21f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediante una iteración por filas, determinamos la categoría asociada a la calidad de\n",
    "# la roca de los pilares y asignamos cada valor a la serie anterior.\n",
    "for i, row_i in enumerate(data.iter_rows(named=True)):\n",
    "    if row_i[\"frec_fracturas\"] <= 1.25:\n",
    "        qa_i = \"Buena calidad\"\n",
    "    elif  row_i[\"frec_fracturas\"] > 2.25:\n",
    "        qa_i = \"Mala calidad\"\n",
    "    else:\n",
    "        qa_i = \"Calidad regular\"\n",
    "    s[i] = qa_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4defb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos la serie anterior a nuestro DataFrame.\n",
    "data = data.with_columns([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f0af880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 8)\n",
      "┌────────────┬────────────┬──────┬──────────┬────────────┬───────────┬─────────────┬───────────────┐\n",
      "│ x          ┆ y          ┆ z    ┆ area     ┆ frec_fract ┆ sigma_z   ┆ tiraje_prom ┆ calidad_roca  │\n",
      "│ ---        ┆ ---        ┆ ---  ┆ ---      ┆ uras       ┆ ---       ┆ ---         ┆ ---           │\n",
      "│ f64        ┆ f64        ┆ i64  ┆ f64      ┆ ---        ┆ f64       ┆ f64         ┆ str           │\n",
      "│            ┆            ┆      ┆          ┆ f64        ┆           ┆             ┆               │\n",
      "╞════════════╪════════════╪══════╪══════════╪════════════╪═══════════╪═════════════╪═══════════════╡\n",
      "│ 776.718035 ┆ 259.185135 ┆ 1125 ┆ 286.5643 ┆ 1.979      ┆ 11.126392 ┆ 122.685     ┆ Calidad       │\n",
      "│            ┆            ┆      ┆          ┆            ┆           ┆             ┆ regular       │\n",
      "│ 811.143435 ┆ 263.059235 ┆ 1125 ┆ 286.5643 ┆ 3.07       ┆ 9.646313  ┆ 121.178     ┆ Mala calidad  │\n",
      "│ 845.568835 ┆ 266.933335 ┆ 1125 ┆ 286.5643 ┆ 3.07       ┆ 10.590594 ┆ 118.942     ┆ Mala calidad  │\n",
      "│ 879.994235 ┆ 270.807435 ┆ 1125 ┆ 286.5643 ┆ 1.75       ┆ 10.605144 ┆ 120.012     ┆ Calidad       │\n",
      "│            ┆            ┆      ┆          ┆            ┆           ┆             ┆ regular       │\n",
      "│ 914.419635 ┆ 274.681535 ┆ 1125 ┆ 286.5643 ┆ 0.67       ┆ 10.059537 ┆ 122.787     ┆ Buena calidad │\n",
      "└────────────┴────────────┴──────┴──────────┴────────────┴───────────┴─────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas de nuestro DataFrame.\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5765c2",
   "metadata": {},
   "source": [
    "Vamos a replicar todo el proceso anterior por medio de una función, a fin de poder medir el tiempo de ejecución de este procedimiento. Haremos un procedimiento similar en **<font color=\"mediumorchid\">Pandas</font>**, de manera tal que podamos comparar los tiempos de ejecución resultantes en ambas librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8664ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metemos todo el proceso anterior vía Polars en una función.\n",
    "def classify_rock_qa_polars(data: pl.DataFrame) -> pd.DataFrame:\n",
    "    # Inicializamos una serie con valores de tipo string iguales a \"a\".\n",
    "    s = pl.Series(values=np.full(fill_value=\"a\", shape=data.shape[0]), name=\"calidad_roca\")\n",
    "    \n",
    "    # Mediante una iteración por filas, determinamos la categoría asociada a la calidad de\n",
    "    # la roca de los pilares y asignamos cada valor a la serie anterior.\n",
    "    for i, row_i in enumerate(data.iter_rows(named=True)):\n",
    "        if row_i[\"frec_fracturas\"] <= 1.25:\n",
    "            qa_i = \"Buena calidad\"\n",
    "        elif  row_i[\"frec_fracturas\"] > 2.25:\n",
    "            qa_i = \"Mala calidad\"\n",
    "        else:\n",
    "            qa_i = \"Calidad regular\"\n",
    "        s[i] = qa_i\n",
    "    \n",
    "    # Asignamos la serie anterior a nuestro DataFrame.\n",
    "    data = data.with_columns([s])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e005995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y también metemos todo en un proceso de Pandas, a fin de comparar.\n",
    "# Una función para iterar por filas en un DataFrame de Pandas y hacer la clasificación.\n",
    "def classify_rock_qa_pandas(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Definimos la nueva columna y la inicializamos con un NaN.\n",
    "    data[\"calidad_roca\"] = np.nan\n",
    "\n",
    "    # Clasificamos los valores de la frecuencia de fracturas mediante un loop.\n",
    "    for row in data.index:\n",
    "        if data.loc[row, \"frec_fracturas\"] <= 1.25:\n",
    "            data.loc[row, \"calidad_roca\"] = \"Buena calidad\"\n",
    "        elif data.loc[row, \"frec_fracturas\"] > 2.25:\n",
    "            data.loc[row, \"calidad_roca\"] = \"Mala calidad\"\n",
    "        else:\n",
    "            data.loc[row, \"calidad_roca\"] = \"Calidad regular\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba7ed941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos en un DataFrame de Pandas.\n",
    "df = pd.read_csv(filepath_or_buffer=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09a16bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.3 ms ± 2.65 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "85.9 ms ± 740 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit classify_rock_qa_polars(data)\n",
    "%timeit classify_rock_qa_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbe259",
   "metadata": {},
   "source": [
    "Vemos pues que, para este caso particular, **<font color=\"mediumorchid\">Polars</font>** es igual de ineficiente que **<font color=\"mediumorchid\">Pandas</font>** a la hora de generar iteraciones por filas sobre DataFrames. Y, además, su sintaxis no resulta en absoluto familiar con respecto a la de **<font color=\"mediumorchid\">Pandas</font>** o, incluso, de **<font color=\"mediumorchid\">Numpy</font>**. Tomaremos ésto como una motivación adicional para hacer todo el esfuerzo posible a fin de vectorizar las operaciones que queramos realizar sobre estructuras de datos de **<font color=\"mediumorchid\">Polars</font>**. La anterior, puntualmente, podemos realizarla fácilmente mediante la función `np.where()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e13a3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos nuevamente nuestra data en un DataFrame de Polars, a fin de partir desde cero.\n",
    "data = pl.read_csv(source=\"datasets/pillars_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddee09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una forma más eficiente de realizar la operación anterior.\n",
    "def classify_rock_qa_vect(data: pl.DataFrame) -> pl.DataFrame:\n",
    "    s = np.where(\n",
    "        data[\"frec_fracturas\"] <= 1.25, \"Buena calidad\",\n",
    "        np.where(data[\"frec_fracturas\"] > 2.25, \"Mala calidad\", \"Calidad regular\")\n",
    "    )\n",
    "    s = pl.Series(values=s)\n",
    "    df = data.with_columns([s.alias(\"calidad_roca\")])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b54fc81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456 µs ± 32 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Revisamos su tiempo de ejecución.\n",
    "%timeit classify_rock_qa_vect(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87353bad",
   "metadata": {},
   "source": [
    "¡Esta función es más de 200 veces más rápida que la anterior, tras haber evitado la iteración por filas!\n",
    "\n",
    "Y ahora sí, ya podemos crear nuestra columna que permite categorizar la calidad de la roca constituyente de nuestros pilares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "813f128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos nuestra función vectorizada.\n",
    "data = classify_rock_qa_vect(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78f830f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 8)\n",
      "┌────────────┬────────────┬──────┬──────────┬────────────┬───────────┬─────────────┬───────────────┐\n",
      "│ x          ┆ y          ┆ z    ┆ area     ┆ frec_fract ┆ sigma_z   ┆ tiraje_prom ┆ calidad_roca  │\n",
      "│ ---        ┆ ---        ┆ ---  ┆ ---      ┆ uras       ┆ ---       ┆ ---         ┆ ---           │\n",
      "│ f64        ┆ f64        ┆ i64  ┆ f64      ┆ ---        ┆ f64       ┆ f64         ┆ str           │\n",
      "│            ┆            ┆      ┆          ┆ f64        ┆           ┆             ┆               │\n",
      "╞════════════╪════════════╪══════╪══════════╪════════════╪═══════════╪═════════════╪═══════════════╡\n",
      "│ 776.718035 ┆ 259.185135 ┆ 1125 ┆ 286.5643 ┆ 1.979      ┆ 11.126392 ┆ 122.685     ┆ Calidad       │\n",
      "│            ┆            ┆      ┆          ┆            ┆           ┆             ┆ regular       │\n",
      "│ 811.143435 ┆ 263.059235 ┆ 1125 ┆ 286.5643 ┆ 3.07       ┆ 9.646313  ┆ 121.178     ┆ Mala calidad  │\n",
      "│ 845.568835 ┆ 266.933335 ┆ 1125 ┆ 286.5643 ┆ 3.07       ┆ 10.590594 ┆ 118.942     ┆ Mala calidad  │\n",
      "│ 879.994235 ┆ 270.807435 ┆ 1125 ┆ 286.5643 ┆ 1.75       ┆ 10.605144 ┆ 120.012     ┆ Calidad       │\n",
      "│            ┆            ┆      ┆          ┆            ┆           ┆             ┆ regular       │\n",
      "│ 914.419635 ┆ 274.681535 ┆ 1125 ┆ 286.5643 ┆ 0.67       ┆ 10.059537 ┆ 122.787     ┆ Buena calidad │\n",
      "└────────────┴────────────┴──────┴──────────┴────────────┴───────────┴─────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas de nuestro DataFrame.\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb6cbf",
   "metadata": {},
   "source": [
    "## Agregaciones por agrupamiento.\n",
    "\n",
    "Cuando empezamos a dar nuestros primeros pasos en **<font color=\"mediumorchid\">Polars</font>**, aprendimos que sus estructuras de datos pueden manipularse por medio de dos conjuntos importantes de transformaciones propias de su lenguaje de dominio específico (DSL), denominadas **contextos** y **expresiones**. Dos de los contextos ya fueron revisados en detalle previamente, siendo usados fundamentalmente para seleccionar datos (mediante los métodos `select()` y  `with_columns()`) y filtrarlos (por medio del método `filter()`). No obstante, nos quedó un contexto pendiente y que nos comprometimos a revisar, y que corresponde a la construcción de **agregaciones vía agrupamientos** por medio de los métodos `groupby()` y `agg()`.\n",
    "\n",
    "Bajo el contexto `groupby()`, podemos trabajar cualquier tipo de expresión de **<font color=\"mediumorchid\">Polars</font>** por medio de grupos construidos a partir de categorías existentes en un DataFrame, de tal forma que podamos generar cualquier tipo de agregación a partir de tales grupos. Por ejemplo, en nuestro DataFrame anterior (`data`), podemos usar el contexto `groupby()` para construir cualquier tipo de agregación en función de las categorías previamente construidas para calificar la calidad del macizo rocoso constituyente de cada pilar. La agregación toma como argumento el nombre de la columna que define a los grupos (en nuestro caso, `\"calidad_roca\"`), y luego hace uso del método `agg()` para generar las agregaciones que queramos, debido a que el resultado de la aplicación del método `groupby()` es un objeto preparado para construir tales agregaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0a66eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<polars.dataframe.groupby.GroupBy at 0x7fd79b50cfd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"calidad_roca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf01825",
   "metadata": {},
   "source": [
    "De esta manera, construiremos la siguiente agregación: Calcularemos el promedio de la carga vertical pre-minería (columna `\"sigma_z\"`) y de la frecuencia de fracturas por metro lineal (columna `\"frec_fracturas\"`), y el máximo valor de tiraje adyacente (columna `\"tiraje_prom\"`) asociados a cada una de las calidades de macizo rocoso previamente definidas. Cada una de estas agregaciones se mostrará con un determinado alias, que definimos en la misma agregación por medio del método `alias()`, de la misma forma en que hicimos con los contextos `select_columns()`, `with_columns()` y `filter()`. La preservación de la **sintaxis contextual** de **<font color=\"mediumorchid\">Polars</font>** es, por supuesto, parte de su lenguaje de dominio específico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d762a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos las agregaciones encadenando al contexto groupby() el método agg().\n",
    "result = data.groupby(\"calidad_roca\").agg(\n",
    "    pl.col(\"sigma_z\").mean().alias(\"avg_sigma_z\"),\n",
    "    pl.col(\"frec_fracturas\").mean().alias(\"avg_frec_fracturas\"),\n",
    "    pl.col(\"tiraje_prom\").max().alias(\"max_tiraje_ady\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d025bf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 4)\n",
      "┌─────────────────┬─────────────┬────────────────────┬────────────────┐\n",
      "│ calidad_roca    ┆ avg_sigma_z ┆ avg_frec_fracturas ┆ max_tiraje_ady │\n",
      "│ ---             ┆ ---         ┆ ---                ┆ ---            │\n",
      "│ str             ┆ f64         ┆ f64                ┆ f64            │\n",
      "╞═════════════════╪═════════════╪════════════════════╪════════════════╡\n",
      "│ Calidad regular ┆ 12.355339   ┆ 1.751776           ┆ 193.471        │\n",
      "│ Buena calidad   ┆ 13.552295   ┆ 0.735853           ┆ 201.307        │\n",
      "│ Mala calidad    ┆ 12.192455   ┆ 3.003736           ┆ 192.489        │\n",
      "└─────────────────┴─────────────┴────────────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b41be",
   "metadata": {},
   "source": [
    "Vemos pues que el método `agg()` es el ideal para construir cualquier tipo de agregación por medio de un agrupamiento previo conforme alguna columna de interés que sea categórica. \n",
    "\n",
    "Existen otras opciones para generar agregaciones de mayor complejidad. Por ejemplo, podemos hacer uso del método `apply()` sobre una agrupación realizada previamente con el contexto `groupby()` para construir un cálculo usando cualquier función que deseemos, similar a lo que ya hemos hecho usando la librería **<font color=\"mediumorchid\">Pandas</font>**. Sin embargo, no lo haremos acá porque, como veremos más adelante, será mucho más eficiente realizar tales cálculos de este tipo aprovechando las **expresiones** provistas por **<font color=\"mediumorchid\">Polars</font>**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d1e16",
   "metadata": {},
   "source": [
    "## Expresiones.\n",
    "\n",
    "El pilar fundamental de la rapidez de **<font color=\"mediumorchid\">Polars</font>** a la hora de realizar cualquier tipo de manipulación de sus estructuras de datos radica en las llamadas **expresiones**, y que, al igual que los contextos, son parte integral de su lenguaje de dominio específico (DSL). Estas expresiones representan una enorme cantidad de operaciones que podemos realizar sobre estructuras de datos de **<font color=\"mediumorchid\">Polars</font>**. Por ejemplo:\n",
    "\n",
    "- Extraer una muestra aleatoria de distintas filas de un DataFrame.\n",
    "- Multiplicar los valores de una columna en un DataFrame por algún escalar.\n",
    "- Extraer una columna que contenga los años que definen los *timestamps* definidos en otra columna en un determinado DataFrame.\n",
    "- Convertir una columna de valores de tipo string en otra que elimine ciertos caracteres problemáticos (por ejemplo, que transforme cualquier caracter en mayúsculas en el mismo, pero en minúsculas).\n",
    "\n",
    "**<font color=\"mediumorchid\">Polars</font>** implementa las operaciones definidas por expresiones muy rápido, paralelizando tales operaciones cuando éstas se definen para varias columnas en un DataFrame. Estas expresiones pueden idearse como aplicaciones cuyo dominio y codominio son series de **<font color=\"mediumorchid\">Polars</font>**, lo que permite manpipular los datos almacenados en un DataFrame encadenando estas expresiones serie a serie (o columna a columna).\n",
    "\n",
    "Al mostrar previamente los contextos de **<font color=\"mediumorchid\">Polars</font>**, ya trabajamos con expresiones, aunque nunca las definimos formalmente. Por ejemplo, definimos el siguiente filtro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83f104bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (12, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>x</th><th>y</th><th>z</th><th>area</th><th>frec_fracturas</th><th>sigma_z</th><th>tiraje_prom</th><th>calidad_roca</th></tr><tr><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>1052.121235</td><td>290.177935</td><td>1125</td><td>286.9684</td><td>3.07</td><td>20.386906</td><td>119.57</td><td>&quot;Mala calidad&quot;</td></tr><tr><td>1086.546635</td><td>294.052035</td><td>1125</td><td>285.655</td><td>3.07</td><td>26.811494</td><td>123.548</td><td>&quot;Mala calidad&quot;</td></tr><tr><td>1080.540535</td><td>280.294935</td><td>1125</td><td>285.7422</td><td>3.07</td><td>24.334748</td><td>122.197</td><td>&quot;Mala calidad&quot;</td></tr><tr><td>1114.965935</td><td>284.169135</td><td>1125</td><td>248.9047</td><td>3.07</td><td>25.943104</td><td>126.022</td><td>&quot;Mala calidad&quot;</td></tr><tr><td>1108.959835</td><td>270.410235</td><td>1125</td><td>286.5643</td><td>3.07</td><td>25.763218</td><td>125.252</td><td>&quot;Mala calidad&quot;</td></tr><tr><td>1102.952735</td><td>256.652235</td><td>1125</td><td>286.5643</td><td>3.07</td><td>21.843244</td><td>126.931</td><td>&quot;Mala calidad&quot;</td></tr><tr><td>1190.809535</td><td>84.682135</td><td>1125</td><td>183.5212</td><td>3.07</td><td>20.67544</td><td>141.254</td><td>&quot;Mala calidad&quot;</td></tr><tr><td>1275.050635</td><td>-95.653065</td><td>1125</td><td>644.9389</td><td>3.07</td><td>22.167532</td><td>148.145</td><td>&quot;Mala calidad&quot;</td></tr><tr><td>1366.417235</td><td>-94.689765</td><td>1125</td><td>557.4125</td><td>3.07</td><td>26.730452</td><td>147.198</td><td>&quot;Mala calidad&quot;</td></tr><tr><td>1336.637035</td><td>-94.482565</td><td>1125</td><td>550.7097</td><td>3.07</td><td>27.14646</td><td>145.482</td><td>&quot;Mala calidad&quot;</td></tr><tr><td>1306.064735</td><td>-94.064965</td><td>1125</td><td>526.795</td><td>3.07</td><td>25.1711</td><td>145.567</td><td>&quot;Mala calidad&quot;</td></tr><tr><td>1335.505435</td><td>-115.879465</td><td>1125</td><td>347.5708</td><td>2.47</td><td>20.350308</td><td>144.021</td><td>&quot;Mala calidad&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (12, 8)\n",
       "┌──────────────┬─────────────┬──────┬──────────┬────────────┬───────────┬─────────────┬────────────┐\n",
       "│ x            ┆ y           ┆ z    ┆ area     ┆ frec_fract ┆ sigma_z   ┆ tiraje_prom ┆ calidad_ro │\n",
       "│ ---          ┆ ---         ┆ ---  ┆ ---      ┆ uras       ┆ ---       ┆ ---         ┆ ca         │\n",
       "│ f64          ┆ f64         ┆ i64  ┆ f64      ┆ ---        ┆ f64       ┆ f64         ┆ ---        │\n",
       "│              ┆             ┆      ┆          ┆ f64        ┆           ┆             ┆ str        │\n",
       "╞══════════════╪═════════════╪══════╪══════════╪════════════╪═══════════╪═════════════╪════════════╡\n",
       "│ 1052.121235  ┆ 290.177935  ┆ 1125 ┆ 286.9684 ┆ 3.07       ┆ 20.386906 ┆ 119.57      ┆ Mala       │\n",
       "│              ┆             ┆      ┆          ┆            ┆           ┆             ┆ calidad    │\n",
       "│ 1086.546635  ┆ 294.052035  ┆ 1125 ┆ 285.655  ┆ 3.07       ┆ 26.811494 ┆ 123.548     ┆ Mala       │\n",
       "│              ┆             ┆      ┆          ┆            ┆           ┆             ┆ calidad    │\n",
       "│ 1080.540535  ┆ 280.294935  ┆ 1125 ┆ 285.7422 ┆ 3.07       ┆ 24.334748 ┆ 122.197     ┆ Mala       │\n",
       "│              ┆             ┆      ┆          ┆            ┆           ┆             ┆ calidad    │\n",
       "│ 1114.965935  ┆ 284.169135  ┆ 1125 ┆ 248.9047 ┆ 3.07       ┆ 25.943104 ┆ 126.022     ┆ Mala       │\n",
       "│              ┆             ┆      ┆          ┆            ┆           ┆             ┆ calidad    │\n",
       "│ …            ┆ …           ┆ …    ┆ …        ┆ …          ┆ …         ┆ …           ┆ …          │\n",
       "│ 1366.417235  ┆ -94.689765  ┆ 1125 ┆ 557.4125 ┆ 3.07       ┆ 26.730452 ┆ 147.198     ┆ Mala       │\n",
       "│              ┆             ┆      ┆          ┆            ┆           ┆             ┆ calidad    │\n",
       "│ 1336.637035  ┆ -94.482565  ┆ 1125 ┆ 550.7097 ┆ 3.07       ┆ 27.14646  ┆ 145.482     ┆ Mala       │\n",
       "│              ┆             ┆      ┆          ┆            ┆           ┆             ┆ calidad    │\n",
       "│ 1306.064735  ┆ -94.064965  ┆ 1125 ┆ 526.795  ┆ 3.07       ┆ 25.1711   ┆ 145.567     ┆ Mala       │\n",
       "│              ┆             ┆      ┆          ┆            ┆           ┆             ┆ calidad    │\n",
       "│ 1335.505435  ┆ -115.879465 ┆ 1125 ┆ 347.5708 ┆ 2.47       ┆ 20.350308 ┆ 144.021     ┆ Mala       │\n",
       "│              ┆             ┆      ┆          ┆            ┆           ┆             ┆ calidad    │\n",
       "└──────────────┴─────────────┴──────┴──────────┴────────────┴───────────┴─────────────┴────────────┘"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un filtro sencillo que se define a partir de una expresión.\n",
    "data.filter((pl.col(\"calidad_roca\") == \"Mala calidad\") & (pl.col(\"sigma_z\") >= 20.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322bbfd",
   "metadata": {},
   "source": [
    "En el filtro anterior, la **expresión** que permite a **<font color=\"mediumorchid\">Polars</font>** transformar el DataFrame `data` en una versión filtrada del mismo es `(pl.col(\"calidad_roca\") == \"Mala calidad\") & (pl.col(\"sigma_z\") >= 20.0)`. Tal expresión es la combinación de otras dos expresiones más simples, que son `pl.col(\"calidad_roca\") == \"Mala calidad\"` y `pl.col(\"sigma_z\") >= 20.0`.\n",
    "\n",
    "Las selecciones de datos en DataFrames también trabajan conforme expresiones, las que a su vez implementan las transformaciones que queremos realizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b896b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las selecciones también trabajan conforme expresiones.\n",
    "data_2 = data.select(\n",
    "    [\n",
    "        (np.sqrt(pl.col(\"x\")**2 + pl.col(\"y\")**2)).alias(\"r\"),\n",
    "        (np.rad2deg(np.arctan(pl.col(\"y\") / pl.col(\"x\")))).alias(\"theta\"),\n",
    "        (pl.col(\"area\") * pl.col(\"frec_fracturas\")).alias(\"p32\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8acd2b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌────────────┬───────────┬────────────┐\n",
      "│ r          ┆ theta     ┆ p32        │\n",
      "│ ---        ┆ ---       ┆ ---        │\n",
      "│ f64        ┆ f64       ┆ f64        │\n",
      "╞════════════╪═══════════╪════════════╡\n",
      "│ 818.821007 ┆ 18.453478 ┆ 567.11075  │\n",
      "│ 852.733155 ┆ 17.968225 ┆ 879.752401 │\n",
      "│ 886.701788 ┆ 17.520121 ┆ 879.752401 │\n",
      "│ 920.720653 ┆ 17.105107 ┆ 501.487525 │\n",
      "│ 954.784382 ┆ 16.719686 ┆ 191.998081 │\n",
      "└────────────┴───────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos las primeras filas de este nuevo DataFrame.\n",
    "print(data_2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd03aa",
   "metadata": {},
   "source": [
    "En la selección anterior, que da lugar a un nuevo DataFrame, las **expresiones** que nos permiten llegar al resultado deseado son las siguientes:\n",
    "\n",
    "- `(np.sqrt(pl.col(\"x\")**2 + pl.col(\"y\")**2)).alias(\"r\")`, que permite construir la coordenada radial $r=x^{2}+y^{2}$ propia de un sistema de coordenadas polares para los pilares. Tal expresión es compuesta, ya que se ha construido a partir de las expresiones `pl.col(\"x\")**2` y `pl.col(\"y\")**2`.\n",
    "- `(np.rad2deg(np.arctan(pl.col(\"y\") / pl.col(\"x\")))).alias(\"theta\")`, que permite construir la coordenada transversal $\\theta = \\arctan(y/x)$ propia de un sistema de coordenadas polares para los pilares. Esta expresión también es compuesta, porque igualmente depende de las expresiones `pl.col(\"x\")**2` y `pl.col(\"y\")**2`.\n",
    "- `(pl.col(\"area\") * pl.col(\"frec_fracturas\")).alias(\"p32\")`, que permite construir una estimación de la frecuencia de fracturas por superficie del pilar completa (en metros cuadrados por metro lineal, y que típicamente se denomina $P_{32}$. Esta expresión también es compuesta, ya que depende de las expresiones `pl.col(\"area\")` y `pl.col(\"frec_fracturas\")`.\n",
    "\n",
    "Cada una de las expresiones anteriores se ejecuta en paralelo, a fin de maximizar la eficiencia en el tiempo de ejecución del contexto completo. Esta es una de las grandes diferencias que tiene **<font color=\"mediumorchid\">Polars</font>** en relación a **<font color=\"mediumorchid\">Pandas</font>**.\n",
    "\n",
    "### Operatoria básica.\n",
    "Con *operaciones básicas* nos referimos al conjunto de todas las operaciones que son expresables por medio de operadores sencillos. Ejemplos de este tipo de operaciones son la suma (`+`), resta (`-`), multiplicación (`*`) y división (`/`) para el caso de **datos de tipo numérico**, y la disyunción (`&`) y conjunción (`|`) lógicas para el caso de **data de tipo string o no numérica**.\n",
    "\n",
    "Vamos a ejemplificar el uso más fundamental de estos operadores para el caso numérico. Para ello, crearemos algunas series de **<font color=\"mediumorchid\">Polars</font>** a partir de arreglos aleatorizados de **<font color=\"mediumorchid\">Numpy</font>**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25a651f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una semilla aleatoria fija.\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf31cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos unos arreglos unidimensionales a partir de los cuales generaremos algunas series.\n",
    "a1 = rng.uniform(low=-1, high=1, size=10)\n",
    "a2 = rng.normal(loc=0, scale=1, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72b9781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos una serie de Polars a partir del arreglo anterior.\n",
    "s1 = pl.Series(values=a1, name=\"x1\")\n",
    "s2 = pl.Series(values=a2, name=\"x2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72f19c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>x1</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>0.547912</td></tr><tr><td>-0.122243</td></tr><tr><td>0.717196</td></tr><tr><td>0.394736</td></tr><tr><td>-0.811645</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5,)\n",
       "Series: 'x1' [f64]\n",
       "[\n",
       "\t0.547912\n",
       "\t-0.122243\n",
       "\t0.717196\n",
       "\t0.394736\n",
       "\t-0.811645\n",
       "]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos las primeras filas de la serie s1 en pantalla.\n",
    "s1.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7260b1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>x2</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>0.879398</td></tr><tr><td>0.777792</td></tr><tr><td>0.066031</td></tr><tr><td>1.127241</td></tr><tr><td>0.467509</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5,)\n",
       "Series: 'x2' [f64]\n",
       "[\n",
       "\t0.879398\n",
       "\t0.777792\n",
       "\t0.066031\n",
       "\t1.127241\n",
       "\t0.467509\n",
       "]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos las primeras filas de la serie s2 en pantalla.\n",
    "s2.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45300a",
   "metadata": {},
   "source": [
    "Luego, para el caso de la suma (`+`) y la multiplicación (`*`), tenemos que:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c89d7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>x1</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>1.42731</td></tr><tr><td>0.655549</td></tr><tr><td>0.783227</td></tr><tr><td>1.521977</td></tr><tr><td>-0.344136</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5,)\n",
       "Series: 'x1' [f64]\n",
       "[\n",
       "\t1.42731\n",
       "\t0.655549\n",
       "\t0.783227\n",
       "\t1.521977\n",
       "\t-0.344136\n",
       "]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suma de las series s1 y s2.\n",
    "(s1 + s2).head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "823ea44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>x1</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>0.481833</td></tr><tr><td>-0.09508</td></tr><tr><td>0.047357</td></tr><tr><td>0.444963</td></tr><tr><td>-0.379452</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5,)\n",
       "Series: 'x1' [f64]\n",
       "[\n",
       "\t0.481833\n",
       "\t-0.09508\n",
       "\t0.047357\n",
       "\t0.444963\n",
       "\t-0.379452\n",
       "]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiplicación de las series s1 y s2.\n",
    "(s1 * s2).head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66acd70d",
   "metadata": {},
   "source": [
    "Podemos observar que, si bien ésto no se explicita, **<font color=\"mediumorchid\">Polars</font>** alinea las correspondientes series en términos de las posiciones relativas que ocupan cada una de sus filas.\n",
    "\n",
    "Las operaciones de este tipo suelen aplicarse conforme el lenguaje de dominio específico de **<font color=\"mediumorchid\">Polars</font>** por medio de expresiones bajo un contexto determinado. Por ejemplo, si creamos un DataFrame a partir de un arreglo bidimensional aleatoriamente definido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c4448a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un arreglo bidimensional aleatorizado (con números normalmente distribuidos).\n",
    "d1 = rng.normal(loc=0, scale=1, size=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea0977c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un DataFrame a partir del arreglo anterior.\n",
    "df1 = pl.DataFrame(data=d1, schema=[f\"x{j}\" for j in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3efb132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 5)\n",
      "┌───────────┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ x1        ┆ x2        ┆ x3        ┆ x4        ┆ x5       │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ f64       ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ -0.184862 ┆ -0.352134 ┆ 2.141648  ┆ 1.128972  ┆ 0.743254 │\n",
      "│ -0.68093  ┆ 0.532309  ┆ -0.406415 ┆ -0.113947 ┆ 0.543154 │\n",
      "│ 1.222541  ┆ 0.365444  ┆ -0.512243 ┆ -0.840156 ┆ -0.66551 │\n",
      "│ -0.154529 ┆ 0.412733  ┆ -0.813773 ┆ -0.824481 ┆ 0.232161 │\n",
      "│ -0.428328 ┆ 0.430821  ┆ 0.615979  ┆ 0.650593  ┆ 0.116686 │\n",
      "└───────────┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos este DataFrame en pantalla.\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d9ef1",
   "metadata": {},
   "source": [
    "Entonces podemos usar expresiones simples para generar las siguientes operaciones aritméticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81ffdfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicación de operaciones sencillas por medio de expresiones encadenadas en un contexto.\n",
    "result = df1.select(\n",
    "    [\n",
    "        # Multiplicamos la columna x1 por 2 y luego le sumamos 10.\n",
    "        (pl.col(\"x1\") * 2 + 10).alias(\"2x1 + 10\"),\n",
    "        # Dividimos las columnas x2 y x4, y el resultado se lo sumamos a x5.\n",
    "        (pl.col(\"x2\") / pl.col(\"x4\") + pl.col(\"x5\")).alias(\"x2/x4 + x5\"),\n",
    "        # Sumamos la columna x1 con el triple de la columna x5, y el resultado\n",
    "        # lo dividimos por 3.\n",
    "        ((pl.col(\"x1\") + 3 * pl.col(\"x5\")) * 3).alias(\"3(x1 + 3x5)\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b6f5019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌───────────┬────────────┬─────────────┐\n",
      "│ 2x1 + 10  ┆ x2/x4 + x5 ┆ 3(x1 + 3x5) │\n",
      "│ ---       ┆ ---        ┆ ---         │\n",
      "│ f64       ┆ f64        ┆ f64         │\n",
      "╞═══════════╪════════════╪═════════════╡\n",
      "│ 9.630275  ┆ 0.431348   ┆ 6.1347      │\n",
      "│ 8.638141  ┆ -4.128378  ┆ 2.8456      │\n",
      "│ 12.445083 ┆ -1.100481  ┆ -2.321963   │\n",
      "│ 9.690941  ┆ -0.268435  ┆ 1.625863    │\n",
      "│ 9.143344  ┆ 0.778883   ┆ -0.234811   │\n",
      "└───────────┴────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb9d31",
   "metadata": {},
   "source": [
    "Las operaciones de comparación también son sencillas de implementar por medio de expresiones. Por ejemplo, para el caso del DataFrame `data`, que contiene la información relativa a los pilares que trabajamos con anterioridad, podemos generar un contexto que admita este tipo de expresiones a fin de construir un resultado Booleano que puede utilizarse fácilmente como filtro para otras operaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32f47f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicación de operaciones de comparación sencillas por medio de expresiones \n",
    "# encadenadas en un contexto.\n",
    "result = data.select(\n",
    "    [\n",
    "        # Todos los pilares cuya área sea menor o igual que 250 m^2.\n",
    "        (pl.col(\"area\") <= 250).alias(\"area <= 250 m^2\"),\n",
    "        # Todos los pilares cuya calidad geotécnica sea mala.\n",
    "        (pl.col(\"calidad_roca\") == \"Mala calidad\").alias(\"calidad == mala\"),\n",
    "        # Todos los pilares cuya carga vertical pre-minería sea mayor que 18 MPa.\n",
    "        (pl.col(\"sigma_z\") > 18.0).alias(\"sigma_z > 18 MPa\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6c4bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────────────┬─────────────────┬──────────────────┐\n",
      "│ area <= 250 m^2 ┆ calidad == mala ┆ sigma_z > 18 MPa │\n",
      "│ ---             ┆ ---             ┆ ---              │\n",
      "│ bool            ┆ bool            ┆ bool             │\n",
      "╞═════════════════╪═════════════════╪══════════════════╡\n",
      "│ false           ┆ false           ┆ false            │\n",
      "│ false           ┆ true            ┆ false            │\n",
      "│ false           ┆ true            ┆ false            │\n",
      "│ false           ┆ false           ┆ false            │\n",
      "│ false           ┆ false           ┆ false            │\n",
      "└─────────────────┴─────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas del resultado anterior.\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c61dd",
   "metadata": {},
   "source": [
    "Por supuesto, es posible construir infinitas expresiones de acuerdo a las condiciones del problema particular que queramos abordar, usando operadores fundamentales tanto aritméticos como de comparación. Los anteriores fueron simplemente ejemplos sencillos de lo mucho que podemos hacer al manipular estructuras de datos vía contextos y expresiones básicas en **<font color=\"mediumorchid\">Polars</font>**.\n",
    "\n",
    "### Funciones aplicables sobre DataFrames.\n",
    "Las expresiones de **<font color=\"mediumorchid\">Polars</font>** nos ofrecen un gran número de funciones y/o rutinas pre-establecidas que nos permiten construir *queries* o consultas de datos de enorme complejidad, sin tener que recurrir a funciones definidas por nosotros y que, en muchos casos, tendrán un tiempo de ejecución nativo de Python y por consiguiente no serán igual de rápidas que las rutinas nativas de **<font color=\"mediumorchid\">Polars</font>**.\n",
    "\n",
    "Estas rutinas son todas aplicables en un contexto de selección de datos (por ejemplo, vía `select()` o `with_columns()`).\n",
    "\n",
    "A fin de aprender algunas de estas funciones, vamos a construir un DataFrame que simulará algunos datos operacionales relativos a una planta de flotación, durante un total de 1000 horas (una hora por registro):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7770a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construiremos el DataFrame a partir de nuestra semilla aleatoria fija.\n",
    "df_flot = pl.DataFrame(\n",
    "    {\n",
    "        \"ley_alim\": rng.normal(loc=0.8, scale=0.2, size=1000),\n",
    "        \"alim_flot\": rng.normal(loc=2000, scale=250, size=1000),\n",
    "        \"sol_alim\": rng.uniform(low=38, high=44, size=1000),\n",
    "        \"num_cells_off\": rng.poisson(lam=0.5, size=1000),\n",
    "        \"niv_med_esp\": rng.uniform(low=10, high=60, size=1000),\n",
    "        \"dif_niv_esp\": rng.uniform(low=-30, high=10, size=1000),\n",
    "        \"ley_conc\": rng.uniform(low=28.5, high=31.5, size=1000),\n",
    "        \"ley_cola\": rng.normal(loc=0.03, scale=0.005, size=1000)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c83917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 8)\n",
      "┌──────────┬─────────────┬───────────┬────────────┬───────────┬─────────────┬───────────┬──────────┐\n",
      "│ ley_alim ┆ alim_flot   ┆ sol_alim  ┆ num_cells_ ┆ niv_med_e ┆ dif_niv_esp ┆ ley_conc  ┆ ley_cola │\n",
      "│ ---      ┆ ---         ┆ ---       ┆ off        ┆ sp        ┆ ---         ┆ ---       ┆ ---      │\n",
      "│ f64      ┆ f64         ┆ f64       ┆ ---        ┆ ---       ┆ f64         ┆ f64       ┆ f64      │\n",
      "│          ┆             ┆           ┆ i64        ┆ f64       ┆             ┆           ┆          │\n",
      "╞══════════╪═════════════╪═══════════╪════════════╪═══════════╪═════════════╪═══════════╪══════════╡\n",
      "│ 0.843738 ┆ 1785.198495 ┆ 43.115736 ┆ 3          ┆ 45.186831 ┆ -24.029143  ┆ 29.189195 ┆ 0.028975 │\n",
      "│ 0.974286 ┆ 1865.584356 ┆ 41.431704 ┆ 0          ┆ 59.507281 ┆ 7.177497    ┆ 29.857525 ┆ 0.026371 │\n",
      "│ 0.844719 ┆ 2135.648616 ┆ 39.282985 ┆ 0          ┆ 22.131268 ┆ -15.446129  ┆ 29.558203 ┆ 0.024677 │\n",
      "│ 0.935783 ┆ 1761.093708 ┆ 43.400548 ┆ 0          ┆ 23.711929 ┆ -25.952022  ┆ 28.854379 ┆ 0.020639 │\n",
      "│ 0.813516 ┆ 2109.378062 ┆ 40.055902 ┆ 0          ┆ 50.676327 ┆ -23.01264   ┆ 31.332929 ┆ 0.023267 │\n",
      "└──────────┴─────────────┴───────────┴────────────┴───────────┴─────────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas de este DataFrame\n",
    "print(df_flot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ff54a",
   "metadata": {},
   "source": [
    "Las columnas de este DataFrame representan la siguiente información:\n",
    "\n",
    "- `ley_alim`: Ley de cobre de alimentación de la planta (% Cu).\n",
    "- `alim_flot`: Tonelaje de mineral que entra a la planta de flotación (tph).\n",
    "- `sol_alim`: Porcentaje de sólidos en la alimentación de la planta (%).\n",
    "- `num_cells_off`: Número de celdas en el circuito que se encuentran fuera de servicio.\n",
    "- `niv_med_esp`: Valor medio de espesor de espuma en el circuito (mm).\n",
    "- `dif_niv_esp`: Diferencia de espesores de espuma entre el primer y el último banco en el circuito (mm).\n",
    "- `ley_conc`: Ley de concentrado resultante del proceso (% Cu).\n",
    "- `ley_cola`: Ley de relave resultante del proceso (% Cu).\n",
    "\n",
    "Crearemos algunas variables categóricas adicionales en este DataFrame, a fin de contar con información numérica que será útil para mostrar la implementación de algunas rutinas nativas de **<font color=\"mediumorchid\">Polars</font>**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97611897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos categorías para leyes de relave.\n",
    "s1 = pl.Series(\n",
    "    values=np.where(\n",
    "        df_flot[\"ley_cola\"] >= 0.038, \"Relaves con alta ley\", \"Relaves controlados\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c70ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos categorías para los niveles de espuma en la pulpa.\n",
    "s2 = pl.Series(\n",
    "    values=np.where(\n",
    "        df_flot[\"dif_niv_esp\"] >= 0, \n",
    "        \"Perfil no recuperativo\",\n",
    "        np.where(\n",
    "            df_flot[\"dif_niv_esp\"] < -15.0,\n",
    "            \"Perfil muy recuperativo\",\n",
    "            \"Perfil medianamente recuperativo\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "550ffcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añádimos esta información a nuestro DataFrame.\n",
    "df_flot = df_flot.with_columns([\n",
    "    s1.alias(\"cond_relaves\"), s2.alias(\"perfil_niveles\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c70aad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ley_alim</th><th>alim_flot</th><th>sol_alim</th><th>num_cells_off</th><th>niv_med_esp</th><th>dif_niv_esp</th><th>ley_conc</th><th>ley_cola</th><th>cond_relaves</th><th>perfil_niveles</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0.843738</td><td>1785.198495</td><td>43.115736</td><td>3</td><td>45.186831</td><td>-24.029143</td><td>29.189195</td><td>0.028975</td><td>&quot;Relaves contro…</td><td>&quot;Perfil muy rec…</td></tr><tr><td>0.974286</td><td>1865.584356</td><td>41.431704</td><td>0</td><td>59.507281</td><td>7.177497</td><td>29.857525</td><td>0.026371</td><td>&quot;Relaves contro…</td><td>&quot;Perfil no recu…</td></tr><tr><td>0.844719</td><td>2135.648616</td><td>39.282985</td><td>0</td><td>22.131268</td><td>-15.446129</td><td>29.558203</td><td>0.024677</td><td>&quot;Relaves contro…</td><td>&quot;Perfil muy rec…</td></tr><tr><td>0.935783</td><td>1761.093708</td><td>43.400548</td><td>0</td><td>23.711929</td><td>-25.952022</td><td>28.854379</td><td>0.020639</td><td>&quot;Relaves contro…</td><td>&quot;Perfil muy rec…</td></tr><tr><td>0.813516</td><td>2109.378062</td><td>40.055902</td><td>0</td><td>50.676327</td><td>-23.01264</td><td>31.332929</td><td>0.023267</td><td>&quot;Relaves contro…</td><td>&quot;Perfil muy rec…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 10)\n",
       "┌──────────┬─────────┬───────────┬────────────┬───┬───────────┬──────────┬────────────┬────────────┐\n",
       "│ ley_alim ┆ alim_fl ┆ sol_alim  ┆ num_cells_ ┆ … ┆ ley_conc  ┆ ley_cola ┆ cond_relav ┆ perfil_niv │\n",
       "│ ---      ┆ ot      ┆ ---       ┆ off        ┆   ┆ ---       ┆ ---      ┆ es         ┆ eles       │\n",
       "│ f64      ┆ ---     ┆ f64       ┆ ---        ┆   ┆ f64       ┆ f64      ┆ ---        ┆ ---        │\n",
       "│          ┆ f64     ┆           ┆ i64        ┆   ┆           ┆          ┆ str        ┆ str        │\n",
       "╞══════════╪═════════╪═══════════╪════════════╪═══╪═══════════╪══════════╪════════════╪════════════╡\n",
       "│ 0.843738 ┆ 1785.19 ┆ 43.115736 ┆ 3          ┆ … ┆ 29.189195 ┆ 0.028975 ┆ Relaves    ┆ Perfil muy │\n",
       "│          ┆ 8495    ┆           ┆            ┆   ┆           ┆          ┆ controlado ┆ recuperati │\n",
       "│          ┆         ┆           ┆            ┆   ┆           ┆          ┆ s          ┆ vo         │\n",
       "│ 0.974286 ┆ 1865.58 ┆ 41.431704 ┆ 0          ┆ … ┆ 29.857525 ┆ 0.026371 ┆ Relaves    ┆ Perfil no  │\n",
       "│          ┆ 4356    ┆           ┆            ┆   ┆           ┆          ┆ controlado ┆ recuperati │\n",
       "│          ┆         ┆           ┆            ┆   ┆           ┆          ┆ s          ┆ vo         │\n",
       "│ 0.844719 ┆ 2135.64 ┆ 39.282985 ┆ 0          ┆ … ┆ 29.558203 ┆ 0.024677 ┆ Relaves    ┆ Perfil muy │\n",
       "│          ┆ 8616    ┆           ┆            ┆   ┆           ┆          ┆ controlado ┆ recuperati │\n",
       "│          ┆         ┆           ┆            ┆   ┆           ┆          ┆ s          ┆ vo         │\n",
       "│ 0.935783 ┆ 1761.09 ┆ 43.400548 ┆ 0          ┆ … ┆ 28.854379 ┆ 0.020639 ┆ Relaves    ┆ Perfil muy │\n",
       "│          ┆ 3708    ┆           ┆            ┆   ┆           ┆          ┆ controlado ┆ recuperati │\n",
       "│          ┆         ┆           ┆            ┆   ┆           ┆          ┆ s          ┆ vo         │\n",
       "│ 0.813516 ┆ 2109.37 ┆ 40.055902 ┆ 0          ┆ … ┆ 31.332929 ┆ 0.023267 ┆ Relaves    ┆ Perfil muy │\n",
       "│          ┆ 8062    ┆           ┆            ┆   ┆           ┆          ┆ controlado ┆ recuperati │\n",
       "│          ┆         ┆           ┆            ┆   ┆           ┆          ┆ s          ┆ vo         │\n",
       "└──────────┴─────────┴───────────┴────────────┴───┴───────────┴──────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos las primeras filas del DataFrame.\n",
    "df_flot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89690574",
   "metadata": {},
   "source": [
    "Una de las rutinas nativas de **<font color=\"mediumorchid\">Polars</font>** más elementales corresponde a `select()`, que sabemos que corresponde al método contextual preferido para seleccionar información de un DataFrame. Es posible seleccionar un número arbitrario de columnas a partir de este contexto, incluyendo todas las columnas del DataFrame en cuestión. Para esto último, basta con usar la expresión `pl.col(\"*\")`, que es equivalente al uso del símbolo `:` en **<font color=\"mediumorchid\">Numpy</font>** para recuperar todos los elementos relativos al eje de un arreglo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a90d1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de todas las columnas en un DataFrame.\n",
    "selection = df_flot.select([pl.col(\"*\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ed1f2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Efectivamente, la selección anterior equivale al mismo DataFrame, ya que tiene sus mismas dimensiones.\n",
    "selection.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38645054",
   "metadata": {},
   "source": [
    "Otra rutina muy usada en **<font color=\"mediumorchid\">Polars</font>** corresponde a `exclude()`, y que nos permite seleccionar un número arbitrario de columnas en un DataFrame, exceptuando a un grupo determinado de columnas que especificamos por su nombre en caso de ser sólo una, o por medio de una lista de Python en el caso de ser más de una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "246cf55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluimos tres columnas de nuestro DataFrame en esta selección.\n",
    "selection = df_flot.select([pl.exclude([\"cond_relaves\", \"perfil_niveles\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6e82b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ley_alim</th><th>alim_flot</th><th>sol_alim</th><th>num_cells_off</th><th>niv_med_esp</th><th>dif_niv_esp</th><th>ley_conc</th><th>ley_cola</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.843738</td><td>1785.198495</td><td>43.115736</td><td>3</td><td>45.186831</td><td>-24.029143</td><td>29.189195</td><td>0.028975</td></tr><tr><td>0.974286</td><td>1865.584356</td><td>41.431704</td><td>0</td><td>59.507281</td><td>7.177497</td><td>29.857525</td><td>0.026371</td></tr><tr><td>0.844719</td><td>2135.648616</td><td>39.282985</td><td>0</td><td>22.131268</td><td>-15.446129</td><td>29.558203</td><td>0.024677</td></tr><tr><td>0.935783</td><td>1761.093708</td><td>43.400548</td><td>0</td><td>23.711929</td><td>-25.952022</td><td>28.854379</td><td>0.020639</td></tr><tr><td>0.813516</td><td>2109.378062</td><td>40.055902</td><td>0</td><td>50.676327</td><td>-23.01264</td><td>31.332929</td><td>0.023267</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "┌──────────┬─────────────┬───────────┬────────────┬───────────┬─────────────┬───────────┬──────────┐\n",
       "│ ley_alim ┆ alim_flot   ┆ sol_alim  ┆ num_cells_ ┆ niv_med_e ┆ dif_niv_esp ┆ ley_conc  ┆ ley_cola │\n",
       "│ ---      ┆ ---         ┆ ---       ┆ off        ┆ sp        ┆ ---         ┆ ---       ┆ ---      │\n",
       "│ f64      ┆ f64         ┆ f64       ┆ ---        ┆ ---       ┆ f64         ┆ f64       ┆ f64      │\n",
       "│          ┆             ┆           ┆ i64        ┆ f64       ┆             ┆           ┆          │\n",
       "╞══════════╪═════════════╪═══════════╪════════════╪═══════════╪═════════════╪═══════════╪══════════╡\n",
       "│ 0.843738 ┆ 1785.198495 ┆ 43.115736 ┆ 3          ┆ 45.186831 ┆ -24.029143  ┆ 29.189195 ┆ 0.028975 │\n",
       "│ 0.974286 ┆ 1865.584356 ┆ 41.431704 ┆ 0          ┆ 59.507281 ┆ 7.177497    ┆ 29.857525 ┆ 0.026371 │\n",
       "│ 0.844719 ┆ 2135.648616 ┆ 39.282985 ┆ 0          ┆ 22.131268 ┆ -15.446129  ┆ 29.558203 ┆ 0.024677 │\n",
       "│ 0.935783 ┆ 1761.093708 ┆ 43.400548 ┆ 0          ┆ 23.711929 ┆ -25.952022  ┆ 28.854379 ┆ 0.020639 │\n",
       "│ 0.813516 ┆ 2109.378062 ┆ 40.055902 ┆ 0          ┆ 50.676327 ┆ -23.01264   ┆ 31.332929 ┆ 0.023267 │\n",
       "└──────────┴─────────────┴───────────┴────────────┴───────────┴─────────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos las primeras filas de la selección anterior.\n",
    "selection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50a446",
   "metadata": {},
   "source": [
    "**<font color=\"mediumorchid\">Polars</font>** también nos provee de rutinas que nos permiten contar el número de valores distintos que puede tomar una variable de tipo categórica. Tales rutinas corresponden a `n_unique()` y `approx_unique()`.\n",
    "\n",
    "El método `n_unique()` nos permite contar el número de valores distintos ocurrentes en cualquier columna de un DataFrame, independiente de su tipo. Naturalmente, es una rutina pensada para la cuantía de valores únicos para variables categóricas u ordinales, aplicable sobretodo para DataFrames con una cantidad no demasiado grande (menor a unos cientos de miles) de filas.\n",
    "\n",
    "Por otro lado, el método `approx_unique()` nos permite hacer virtualmente lo mismo que con `n_unique()`. La diferencia es que `approx_unique()`, como su nombre lo sugiere, es una rutina que estima un valor aproximado de los valores únicos que toma una columna, lo que resulta muy útil en el contexto de DataFrames que almacenan millones de registros o más. Se trata pues de una rutina pensada para un contexto de *big data*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63325f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuantía de valores únicos mediante n_unique().\n",
    "counts = df_flot.select(\n",
    "    [\n",
    "        pl.col(\"cond_relaves\").n_unique().alias(\"num_cond_relaves\"),\n",
    "        pl.col(\"perfil_niveles\").n_unique().alias(\"num_perfiles\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "761b884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 2)\n",
      "┌──────────────────┬──────────────┐\n",
      "│ num_cond_relaves ┆ num_perfiles │\n",
      "│ ---              ┆ ---          │\n",
      "│ u32              ┆ u32          │\n",
      "╞══════════════════╪══════════════╡\n",
      "│ 2                ┆ 3            │\n",
      "└──────────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a8cd9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuantía de valores únicos mediante approx_unique().\n",
    "counts = df_flot.select(\n",
    "    [\n",
    "        pl.col(\"cond_relaves\").approx_unique().alias(\"num_cond_relaves\"),\n",
    "        pl.col(\"perfil_niveles\").approx_unique().alias(\"num_perfiles\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc191215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 2)\n",
      "┌──────────────────┬──────────────┐\n",
      "│ num_cond_relaves ┆ num_perfiles │\n",
      "│ ---              ┆ ---          │\n",
      "│ u32              ┆ u32          │\n",
      "╞══════════════════╪══════════════╡\n",
      "│ 2                ┆ 3            │\n",
      "└──────────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f40f7",
   "metadata": {},
   "source": [
    "En una situación para la cual los DataFrames que manipulamos no tienen una enorme cantidad de registros, como ocurre en un contexto de *big data*, `n_unique()` y `approx_unique()` nos darán los mismos resultados, y con tiempos de ejecución similares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b6e8de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.2 µs ± 4.27 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "95.9 µs ± 2.78 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df_flot.select([pl.col(\"cond_relaves\").n_unique().alias(\"num_cond_relaves\"), pl.col(\"perfil_niveles\").n_unique().alias(\"num_perfiles\")])\n",
    "%timeit df_flot.select([pl.col(\"cond_relaves\").approx_unique().alias(\"num_cond_relaves\"), pl.col(\"perfil_niveles\").approx_unique().alias(\"num_perfiles\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c748b",
   "metadata": {},
   "source": [
    "Por lo tanto, es normal que en un contexto minero, simplemente nos limitemos a utilizar `n_unique()` para realizar cuantías como las anteriores.\n",
    "\n",
    "Otra rutina de cuantía muy común es `value_counts()`, y que nos permite contar el número de filas en un DataFrame para el cual el valor de una determinada columna se repite. También es útil para saber cómo se estratifica un dataset en función de ciertas variables categóricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4a6b625a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cuantia_instancias_por_perfil</th></tr><tr><td>struct[2]</td></tr></thead><tbody><tr><td>{&quot;Perfil no recuperativo&quot;,245}</td></tr><tr><td>{&quot;Perfil medianamente recuperativo&quot;,379}</td></tr><tr><td>{&quot;Perfil muy recuperativo&quot;,376}</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 1)\n",
       "┌───────────────────────────────────┐\n",
       "│ cuantia_instancias_por_perfil     │\n",
       "│ ---                               │\n",
       "│ struct[2]                         │\n",
       "╞═══════════════════════════════════╡\n",
       "│ {\"Perfil no recuperativo\",245}    │\n",
       "│ {\"Perfil medianamente recuperati… │\n",
       "│ {\"Perfil muy recuperativo\",376}   │\n",
       "└───────────────────────────────────┘"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generamos una cuantía de instancias por categoría.\n",
    "df_flot.select(\n",
    "    [\n",
    "        pl.col(\"perfil_niveles\").value_counts().alias(\"cuantia_instancias_por_perfil\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184101a8",
   "metadata": {},
   "source": [
    "La expresión `value_counts()` retorna un DataFrame con un mapeo de las cuantías requeridas (del tipo `Struct`), que se lee como `{valor: número de ocurrencias}`.\n",
    "\n",
    "**<font color=\"mediumorchid\">Polars</font>** soporta igualmente sentencias condicionales que son propias de la sintaxis de su lenguaje específico, y se resumen por medio de los métodos `when()`, `then()` y `otherwise()`.\n",
    "\n",
    "El método `when()` nos permite **evaluar una determinada condición**, que luego puede ser utilizada para implementar alguna expresión por medio del método `then()`. Cuando la condición especificada por medio de `when()` no se cumple, podemos implementar igualmente alguna expresión por medio del método `otherwise()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7082fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una columna categórica que evalúe las condiciones de las leyes de alimentación.\n",
    "feed_cats = df_flot.select(\n",
    "    [\n",
    "        pl.col(\"ley_alim\"),\n",
    "        pl.when(pl.col(\"ley_alim\") <= 0.70) # La condición sobre la cual trabajaremos.\n",
    "        .then(pl.lit(\"Baja ley\")) # Valor si la condición se cumple.\n",
    "        .otherwise(pl.lit(\"Ley normal\")) # Valor si la condición no se cumple.\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b2c8ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (8, 2)\n",
      "┌──────────┬────────────┐\n",
      "│ ley_alim ┆ literal    │\n",
      "│ ---      ┆ ---        │\n",
      "│ f64      ┆ str        │\n",
      "╞══════════╪════════════╡\n",
      "│ 0.843738 ┆ Ley normal │\n",
      "│ 0.974286 ┆ Ley normal │\n",
      "│ 0.844719 ┆ Ley normal │\n",
      "│ 0.935783 ┆ Ley normal │\n",
      "│ 0.813516 ┆ Ley normal │\n",
      "│ 0.857824 ┆ Ley normal │\n",
      "│ 0.926258 ┆ Ley normal │\n",
      "│ 0.508569 ┆ Baja ley   │\n",
      "└──────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras 8 filas de este resultado.\n",
    "print(feed_cats.head(n=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9296d",
   "metadata": {},
   "source": [
    "Observemos que las expresiones condicionales encadenadas por medio de `when()` trabajan de la misma forma que la función `numpy.where()`, pero aprovechando el lenguaje específico de dominio de **<font color=\"mediumorchid\">Polars</font>** a fin de lograr un tiempo de ejecución más rápido. De hecho, es posible anidar varias condiciones de tipo `when()`, usualmente especificándolas en la expresión `otherwise()`. Para comparar ambas alternativas, crearemos dos funciones que construirán la columna `\"perfil_niveles\"`, y que ya habíamos construido rápidamente en forma previa mediante la función `numpy.where()`. Esta columna nos permitirá establecer la condición de los perfiles de rendimiento del circuito de flotación representado en el DataFrame, considerando las siguientes condiciones:\n",
    "\n",
    "- Si la diferencia de niveles de espuma no es negativa (lo que implica que los niveles de espuma a lo largo del circuito no decrecen), el perfil se definirá como **no recuperativo**.\n",
    "- Si la diferencia de niveles de espuma es menor que -15 mm, el perfil se definirá como **muy recuperativo**.\n",
    "- Valores intermedios de la diferencia de pefiles (entre -15 mm y 0) serán categorizados como **medianamente recuperativos**.\n",
    "\n",
    "De esta manera, tenemos que:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "817148cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función que permite construir la columna \"perfil_niveles\" mediante la función\n",
    "# numpy.where(), para luego comparar su rendimiento con el método when() de Polars.\n",
    "def create_level_conditions_where(data: pl.DataFrame) -> pl.DataFrame:\n",
    "    s = pl.Series(\n",
    "        values=np.where(\n",
    "            df_flot[\"dif_niv_esp\"] >= 0, \n",
    "            \"Perfil no recuperativo\",\n",
    "            np.where(\n",
    "                df_flot[\"dif_niv_esp\"] < -15.0,\n",
    "                \"Perfil muy recuperativo\",\n",
    "                \"Perfil medianamente recuperativo\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    df = data.with_columns([s.alias(\"perfil_niveles\")])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "644567d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función que hará exactamente lo mismo, pero usando el método when().\n",
    "def create_level_conditions_when(data: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = data.with_columns(\n",
    "        [\n",
    "            pl.when(pl.col(\"dif_niv_esp\") >= 0)\n",
    "            .then(pl.lit(\"Perfil no recuperativo\"))\n",
    "            .otherwise(\n",
    "                pl.when(pl.col(\"dif_niv_esp\") < -15.0)\n",
    "                .then(pl.lit(\"Perfil muy recuperativo\"))\n",
    "                .otherwise(pl.lit(\"Perfil medianamente recuperativo\"))\n",
    "            ).alias(\"perfil_niveles\")\n",
    "        ]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f5303",
   "metadata": {},
   "source": [
    "Como ya creamos esta columna antes, la eliminaremos para testear nuestras funciones. **Para eliminar una columna de un DataFrame**, basta con utilizar el método `drop()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "50729e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la columna \"perfil_niveles\".\n",
    "df_flot = df_flot.drop(\"perfil_niveles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ea5d85d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ley_alim</th><th>alim_flot</th><th>sol_alim</th><th>num_cells_off</th><th>niv_med_esp</th><th>dif_niv_esp</th><th>ley_conc</th><th>ley_cola</th><th>cond_relaves</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>0.843738</td><td>1785.198495</td><td>43.115736</td><td>3</td><td>45.186831</td><td>-24.029143</td><td>29.189195</td><td>0.028975</td><td>&quot;Relaves contro…</td></tr><tr><td>0.974286</td><td>1865.584356</td><td>41.431704</td><td>0</td><td>59.507281</td><td>7.177497</td><td>29.857525</td><td>0.026371</td><td>&quot;Relaves contro…</td></tr><tr><td>0.844719</td><td>2135.648616</td><td>39.282985</td><td>0</td><td>22.131268</td><td>-15.446129</td><td>29.558203</td><td>0.024677</td><td>&quot;Relaves contro…</td></tr><tr><td>0.935783</td><td>1761.093708</td><td>43.400548</td><td>0</td><td>23.711929</td><td>-25.952022</td><td>28.854379</td><td>0.020639</td><td>&quot;Relaves contro…</td></tr><tr><td>0.813516</td><td>2109.378062</td><td>40.055902</td><td>0</td><td>50.676327</td><td>-23.01264</td><td>31.332929</td><td>0.023267</td><td>&quot;Relaves contro…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌──────────┬──────────┬───────────┬────────────┬───┬───────────┬───────────┬──────────┬────────────┐\n",
       "│ ley_alim ┆ alim_flo ┆ sol_alim  ┆ num_cells_ ┆ … ┆ dif_niv_e ┆ ley_conc  ┆ ley_cola ┆ cond_relav │\n",
       "│ ---      ┆ t        ┆ ---       ┆ off        ┆   ┆ sp        ┆ ---       ┆ ---      ┆ es         │\n",
       "│ f64      ┆ ---      ┆ f64       ┆ ---        ┆   ┆ ---       ┆ f64       ┆ f64      ┆ ---        │\n",
       "│          ┆ f64      ┆           ┆ i64        ┆   ┆ f64       ┆           ┆          ┆ str        │\n",
       "╞══════════╪══════════╪═══════════╪════════════╪═══╪═══════════╪═══════════╪══════════╪════════════╡\n",
       "│ 0.843738 ┆ 1785.198 ┆ 43.115736 ┆ 3          ┆ … ┆ -24.02914 ┆ 29.189195 ┆ 0.028975 ┆ Relaves    │\n",
       "│          ┆ 495      ┆           ┆            ┆   ┆ 3         ┆           ┆          ┆ controlado │\n",
       "│          ┆          ┆           ┆            ┆   ┆           ┆           ┆          ┆ s          │\n",
       "│ 0.974286 ┆ 1865.584 ┆ 41.431704 ┆ 0          ┆ … ┆ 7.177497  ┆ 29.857525 ┆ 0.026371 ┆ Relaves    │\n",
       "│          ┆ 356      ┆           ┆            ┆   ┆           ┆           ┆          ┆ controlado │\n",
       "│          ┆          ┆           ┆            ┆   ┆           ┆           ┆          ┆ s          │\n",
       "│ 0.844719 ┆ 2135.648 ┆ 39.282985 ┆ 0          ┆ … ┆ -15.44612 ┆ 29.558203 ┆ 0.024677 ┆ Relaves    │\n",
       "│          ┆ 616      ┆           ┆            ┆   ┆ 9         ┆           ┆          ┆ controlado │\n",
       "│          ┆          ┆           ┆            ┆   ┆           ┆           ┆          ┆ s          │\n",
       "│ 0.935783 ┆ 1761.093 ┆ 43.400548 ┆ 0          ┆ … ┆ -25.95202 ┆ 28.854379 ┆ 0.020639 ┆ Relaves    │\n",
       "│          ┆ 708      ┆           ┆            ┆   ┆ 2         ┆           ┆          ┆ controlado │\n",
       "│          ┆          ┆           ┆            ┆   ┆           ┆           ┆          ┆ s          │\n",
       "│ 0.813516 ┆ 2109.378 ┆ 40.055902 ┆ 0          ┆ … ┆ -23.01264 ┆ 31.332929 ┆ 0.023267 ┆ Relaves    │\n",
       "│          ┆ 062      ┆           ┆            ┆   ┆           ┆           ┆          ┆ controlado │\n",
       "│          ┆          ┆           ┆            ┆   ┆           ┆           ┆          ┆ s          │\n",
       "└──────────┴──────────┴───────────┴────────────┴───┴───────────┴───────────┴──────────┴────────────┘"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chequeamos que esta columna ya no existe imprimiendo en pantalla nuestro DataFrame.\n",
    "df_flot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b4757",
   "metadata": {},
   "source": [
    "Y vemos que la columna `\"perfil_niveles\"` ya no existe. Notemos que el método `drop()`, a diferencia de lo que ocurre en **<font color=\"mediumorchid\">Pandas</font>**, no requiere de un parámetro `axis` que especifique el eje de la tabla respecto del cual eliminamos parte de la misma. Por lo tanto, `drop()` únicamente elimina columnas en un DataFrame. Y no realiza la modificación en el acto, razón por la cual asignamos el resultado de la eliminación a una variable con el mismo nombre (es decir, no disponemos de un parámetro `in_place`).\n",
    "\n",
    "Por cierto, si deseamos eliminar más de una columna, basta con especificar sus nombres por medio de una lista de Python.\n",
    "\n",
    "Ahora sí testeamos nuestras funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d4d415ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558 µs ± 20.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "223 µs ± 1.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit create_level_conditions_where(df_flot)\n",
    "%timeit create_level_conditions_when(df_flot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d387ca00",
   "metadata": {},
   "source": [
    "Y observamos que la creación de la columna categórica `\"perfil_niveles\"` se realiza el doble de rápido por medio de una expresión de tipo `when()` que cuando se realiza esto mismo por medio de la función `numpy.where()`. Esto no es menor, ya que las rutinas de **<font color=\"mediumorchid\">Numpy</font>** son vectorizadas y muy rápidas. **<font color=\"mediumorchid\">Polars</font>** es capaz de hacer el trabajo incluso más rápido que **<font color=\"mediumorchid\">Numpy</font>** cuando nos apegamos a la estructura de su lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3fe32eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la columna nuevamente, usando nuestra función creada íntegramente en Polars.\n",
    "df_flot = create_level_conditions_when(df_flot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "290317e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ley_alim</th><th>alim_flot</th><th>sol_alim</th><th>num_cells_off</th><th>niv_med_esp</th><th>dif_niv_esp</th><th>ley_conc</th><th>ley_cola</th><th>cond_relaves</th><th>perfil_niveles</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0.843738</td><td>1785.198495</td><td>43.115736</td><td>3</td><td>45.186831</td><td>-24.029143</td><td>29.189195</td><td>0.028975</td><td>&quot;Relaves contro…</td><td>&quot;Perfil muy rec…</td></tr><tr><td>0.974286</td><td>1865.584356</td><td>41.431704</td><td>0</td><td>59.507281</td><td>7.177497</td><td>29.857525</td><td>0.026371</td><td>&quot;Relaves contro…</td><td>&quot;Perfil no recu…</td></tr><tr><td>0.844719</td><td>2135.648616</td><td>39.282985</td><td>0</td><td>22.131268</td><td>-15.446129</td><td>29.558203</td><td>0.024677</td><td>&quot;Relaves contro…</td><td>&quot;Perfil muy rec…</td></tr><tr><td>0.935783</td><td>1761.093708</td><td>43.400548</td><td>0</td><td>23.711929</td><td>-25.952022</td><td>28.854379</td><td>0.020639</td><td>&quot;Relaves contro…</td><td>&quot;Perfil muy rec…</td></tr><tr><td>0.813516</td><td>2109.378062</td><td>40.055902</td><td>0</td><td>50.676327</td><td>-23.01264</td><td>31.332929</td><td>0.023267</td><td>&quot;Relaves contro…</td><td>&quot;Perfil muy rec…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 10)\n",
       "┌──────────┬─────────┬───────────┬────────────┬───┬───────────┬──────────┬────────────┬────────────┐\n",
       "│ ley_alim ┆ alim_fl ┆ sol_alim  ┆ num_cells_ ┆ … ┆ ley_conc  ┆ ley_cola ┆ cond_relav ┆ perfil_niv │\n",
       "│ ---      ┆ ot      ┆ ---       ┆ off        ┆   ┆ ---       ┆ ---      ┆ es         ┆ eles       │\n",
       "│ f64      ┆ ---     ┆ f64       ┆ ---        ┆   ┆ f64       ┆ f64      ┆ ---        ┆ ---        │\n",
       "│          ┆ f64     ┆           ┆ i64        ┆   ┆           ┆          ┆ str        ┆ str        │\n",
       "╞══════════╪═════════╪═══════════╪════════════╪═══╪═══════════╪══════════╪════════════╪════════════╡\n",
       "│ 0.843738 ┆ 1785.19 ┆ 43.115736 ┆ 3          ┆ … ┆ 29.189195 ┆ 0.028975 ┆ Relaves    ┆ Perfil muy │\n",
       "│          ┆ 8495    ┆           ┆            ┆   ┆           ┆          ┆ controlado ┆ recuperati │\n",
       "│          ┆         ┆           ┆            ┆   ┆           ┆          ┆ s          ┆ vo         │\n",
       "│ 0.974286 ┆ 1865.58 ┆ 41.431704 ┆ 0          ┆ … ┆ 29.857525 ┆ 0.026371 ┆ Relaves    ┆ Perfil no  │\n",
       "│          ┆ 4356    ┆           ┆            ┆   ┆           ┆          ┆ controlado ┆ recuperati │\n",
       "│          ┆         ┆           ┆            ┆   ┆           ┆          ┆ s          ┆ vo         │\n",
       "│ 0.844719 ┆ 2135.64 ┆ 39.282985 ┆ 0          ┆ … ┆ 29.558203 ┆ 0.024677 ┆ Relaves    ┆ Perfil muy │\n",
       "│          ┆ 8616    ┆           ┆            ┆   ┆           ┆          ┆ controlado ┆ recuperati │\n",
       "│          ┆         ┆           ┆            ┆   ┆           ┆          ┆ s          ┆ vo         │\n",
       "│ 0.935783 ┆ 1761.09 ┆ 43.400548 ┆ 0          ┆ … ┆ 28.854379 ┆ 0.020639 ┆ Relaves    ┆ Perfil muy │\n",
       "│          ┆ 3708    ┆           ┆            ┆   ┆           ┆          ┆ controlado ┆ recuperati │\n",
       "│          ┆         ┆           ┆            ┆   ┆           ┆          ┆ s          ┆ vo         │\n",
       "│ 0.813516 ┆ 2109.37 ┆ 40.055902 ┆ 0          ┆ … ┆ 31.332929 ┆ 0.023267 ┆ Relaves    ┆ Perfil muy │\n",
       "│          ┆ 8062    ┆           ┆            ┆   ┆           ┆          ┆ controlado ┆ recuperati │\n",
       "│          ┆         ┆           ┆            ┆   ┆           ┆          ┆ s          ┆ vo         │\n",
       "└──────────┴─────────┴───────────┴────────────┴───┴───────────┴──────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos las primeras filas de nuestro resultado.\n",
    "df_flot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3642fb4",
   "metadata": {},
   "source": [
    "Notemos que, al usar la expresión `when()`, hemos escrito los cambios relativos a las condiciones así definidas en una nueva columna por medio de la función `pl.lit()`.\n",
    "\n",
    "### Cambios en los tipos de datos.\n",
    "Las expresiones que permiten transformar los datos relativos a una columna en un DataFrame en datos de otro tipo se agrupan en los llamados **métodos de casting**. En términos literales, *casting* es la acción de transformar un tipo de dato en otro. **<font color=\"mediumorchid\">Polars</font>** hace uso de Arrow para gestionar la data almacenada en la memoria de nuestro sistema y permite que la implementación nativa en Rust realice las conversiones de tipo. Tales conversiones se aglutinan en el método `cast()`.\n",
    "\n",
    "El método `cast()` incluye un parámetro Booleano llamado `strict`, y que determina la forma en la cual **<font color=\"mediumorchid\">Polars</font>** se comportará cuando se encuentre con algún valor que no pueda ser transformado desde el tipo (`DateType`) de origen al tipo (`DateType`) objetivo. Por defecto, `cast()` se setea con `strict=True`, lo que significa que **<font color=\"mediumorchid\">Polars</font>** levantará un error (y nos notificará al respecto) cuando la conversión no pueda realizarse. Por otro lado, si seteamos `strict=False`, **<font color=\"mediumorchid\">Polars</font>** transformará cualquier valor problemático en un registro vacío, que en esta librería se denomina `null` (y es equivalente a lo que, en **<font color=\"mediumorchid\">Pandas</font>**, conocemos como `nan`).\n",
    "\n",
    "#### Caso 1: Datos numéricos.\n",
    "Para ejemplificar el *casting* de datos numéricos, crearemos un DataFrame acorde, que contendrá columnas numéricas de distintos tipos (conforme los tipos de Arrow). Este es el mismo ejemplo que podemos ver en la [documentación](https://pola-rs.github.io/polars-book/user-guide/expressions/casting/#numerics) de **<font color=\"mediumorchid\">Polars</font>**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb2305d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro DataFrame para jugar un poco con las conversiones de datos.\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"integers\": [1, 2, 3, 4, 5],\n",
    "        \"big_integers\": [1, 10000002, 3, 10000004, 10000005],\n",
    "        \"floats\": [4.0, 5.0, 6.0, 7.0, 8.0],\n",
    "        \"floats_with_decimal\": [4.532, 5.5, 6.5, 7.5, 8.5],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "48c1e866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 4)\n",
      "┌──────────┬──────────────┬────────┬─────────────────────┐\n",
      "│ integers ┆ big_integers ┆ floats ┆ floats_with_decimal │\n",
      "│ ---      ┆ ---          ┆ ---    ┆ ---                 │\n",
      "│ i64      ┆ i64          ┆ f64    ┆ f64                 │\n",
      "╞══════════╪══════════════╪════════╪═════════════════════╡\n",
      "│ 1        ┆ 1            ┆ 4.0    ┆ 4.532               │\n",
      "│ 2        ┆ 10000002     ┆ 5.0    ┆ 5.5                 │\n",
      "│ 3        ┆ 3            ┆ 6.0    ┆ 6.5                 │\n",
      "│ 4        ┆ 10000004     ┆ 7.0    ┆ 7.5                 │\n",
      "│ 5        ┆ 10000005     ┆ 8.0    ┆ 8.5                 │\n",
      "└──────────┴──────────────┴────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos nuestro DataFrame en pantalla.\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab722c",
   "metadata": {},
   "source": [
    "Para realizar conversiones de tipo entre números enteros y de punto flotante, y viceversa, aplicamos el método `cast()`. Estas conversiones se realizan por medio de expresiones de **<font color=\"mediumorchid\">Polars</font>** en un contexto determinado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8ad3f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión de números de punto flotante a enteros y viceversa.\n",
    "result = df.select(\n",
    "    [\n",
    "        pl.col(\"integers\").cast(pl.Float32).alias(\"enteros_a_flotantes\"),\n",
    "        pl.col(\"floats\").cast(pl.Int32).alias(\"flotantes_a_enteros\"),\n",
    "        pl.col(\"floats_with_decimal\").cast(pl.Int32).alias(\"decimales_a_enteros\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "856054f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────────────────┬─────────────────────┬─────────────────────┐\n",
      "│ enteros_a_flotantes ┆ flotantes_a_enteros ┆ decimales_a_enteros │\n",
      "│ ---                 ┆ ---                 ┆ ---                 │\n",
      "│ f32                 ┆ i32                 ┆ i32                 │\n",
      "╞═════════════════════╪═════════════════════╪═════════════════════╡\n",
      "│ 1.0                 ┆ 4                   ┆ 4                   │\n",
      "│ 2.0                 ┆ 5                   ┆ 5                   │\n",
      "│ 3.0                 ┆ 6                   ┆ 6                   │\n",
      "│ 4.0                 ┆ 7                   ┆ 7                   │\n",
      "│ 5.0                 ┆ 8                   ┆ 8                   │\n",
      "└─────────────────────┴─────────────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17726382",
   "metadata": {},
   "source": [
    "Las conversiones anteriores truncan cualquier decimal al llevar valores de punto flotante a enteros.\n",
    "\n",
    "El *casting* de columnas en un DataFrame puede resultar útil para reducir el consumo de memoria por efecto de la manipulación de datos en una estructura de tamaño muy grande. Para ello, podemos modificar el número de bits asociados a la representación de un número (cuando ésto no sea un problema). En el siguiente bloque de código, generamos un **downcast** de `Int32` a `Int16` y de `Float64` a `Float32`, a fin de reducir el consumo de memoria en manipulaciones sucesivas de nuestro DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c6a04c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downcast en datos numéricos.\n",
    "result = df.select(\n",
    "    [\n",
    "        pl.col(\"integers\").cast(pl.Int16).alias(\"enteros_de_16bits\"),\n",
    "        pl.col(\"floats\").cast(pl.Float32).alias(\"flotantes_de_32bits\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e234d22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌───────────────────┬─────────────────────┐\n",
      "│ enteros_de_16bits ┆ flotantes_de_32bits │\n",
      "│ ---               ┆ ---                 │\n",
      "│ i16               ┆ f32                 │\n",
      "╞═══════════════════╪═════════════════════╡\n",
      "│ 1                 ┆ 4.0                 │\n",
      "│ 2                 ┆ 5.0                 │\n",
      "│ 3                 ┆ 6.0                 │\n",
      "│ 4                 ┆ 7.0                 │\n",
      "│ 5                 ┆ 8.0                 │\n",
      "└───────────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fa623",
   "metadata": {},
   "source": [
    "Al implementar cualquier tipo de downcast en un DataFrame, es muy importante asegurarnos que el número de bits escogidos (por ejemplo, 64, 32 o 16) sea suficiente para representar los números más pequeños y más grandes en una determinada columna. Por ejemplo, el uso de un tipo entero de 32 bits (`Int32`) nos permite manipular enteros en el rango `[-2147483648, 2147483647]`, mientras que el tipo entero de 8 bits (`Int8`) cubre enteros en el rango `[-128, 127]`. Intentar la implementación de cualquier downcast en un DataFrame cuyos valores, en la columna de interés, exceden el rango cubierto por el tipo de dato requerido generará que **<font color=\"mediumorchid\">Polars</font>** levante un error (de tipo `ComputeError`). Cuando esto ocurre, hablamos de un problema de **overflow** en las conversiones de tipos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "056fd37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strict conversion from `i64` to `i8` failed for value(s) [10000002, 10000004, 10000005]; if you were trying to cast Utf8 to temporal dtypes, consider using `strptime`\n"
     ]
    }
   ],
   "source": [
    "# Un problema de overflow.\n",
    "try:\n",
    "    out = df.select([pl.col(\"big_integers\").cast(pl.Int8)])\n",
    "    print(out)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3db68",
   "metadata": {},
   "source": [
    "Cuando ocurren problemas de overflow, y no queremos conservar los valores problemáticos, podemos setear el parámetro `strict=False`, a fin de que **<font color=\"mediumorchid\">Polars</font>** transforme todos los valores problemáticos en `null`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "76d19034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos valores no convertibles según el tipo objetivo a null.\n",
    "result = df.select([pl.col(\"big_integers\").cast(pl.Int8, strict=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2ee13de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 1)\n",
      "┌──────────────┐\n",
      "│ big_integers │\n",
      "│ ---          │\n",
      "│ i8           │\n",
      "╞══════════════╡\n",
      "│ 1            │\n",
      "│ null         │\n",
      "│ 3            │\n",
      "│ null         │\n",
      "│ null         │\n",
      "└──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9555ffe2",
   "metadata": {},
   "source": [
    "Recordemos que `null` es una representación de carencia de datos en una posición de un DataFrame, similar a la que hacemos en **<font color=\"mediumorchid\">Pandas</font>** por medio de `nan`.\n",
    "\n",
    "#### Caso 2: Cambios en strings.\n",
    "Es posible generar castings en datos de tipo string y convertirlos en datos numéricos, y viceversa, mediante el método `cast()` (siempre que aquello tenga sentido). Para ejemplificar este caso, construiremos un DataFrame que será útil para mostrar este tipo de conversiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d111a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro DataFrame para jugar con las conversiones de datos aplicables a strings.\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"integers\": [1, 2, 3, 4, 5],\n",
    "        \"float\": [4.0, 5.03, 6.0, 7.0, 8.0],\n",
    "        \"floats_as_string\": [\"4.0\", \"5.0\", \"6.0\", \"7.0\", \"8.0\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c52338",
   "metadata": {},
   "source": [
    "Haremos la siguiente conversión: Las columnas `\"integers\"` y `\"float\"` serán convertidas a strings de tipo `Utf8` (encoding de Arrow de tipo UTF de 8 bits), y la columna `\"floats_as_string\"` será convertida en un tipo flotante de 64 bits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ad93e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos las conversiones de string a números y de números a string.\n",
    "result = df.select(\n",
    "    [\n",
    "        pl.col(\"integers\").cast(pl.Utf8).alias(\"enteros_a_strings\"),\n",
    "        pl.col(\"float\").cast(pl.Utf8).alias(\"flotantes_a_strings\"),\n",
    "        pl.col(\"floats_as_string\").cast(pl.Float64).alias(\"strings_a_flotantes\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "48747adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌───────────────────┬─────────────────────┬─────────────────────┐\n",
      "│ enteros_a_strings ┆ flotantes_a_strings ┆ strings_a_flotantes │\n",
      "│ ---               ┆ ---                 ┆ ---                 │\n",
      "│ str               ┆ str                 ┆ f64                 │\n",
      "╞═══════════════════╪═════════════════════╪═════════════════════╡\n",
      "│ 1                 ┆ 4.0                 ┆ 4.0                 │\n",
      "│ 2                 ┆ 5.03                ┆ 5.0                 │\n",
      "│ 3                 ┆ 6.0                 ┆ 6.0                 │\n",
      "│ 4                 ┆ 7.0                 ┆ 7.0                 │\n",
      "│ 5                 ┆ 8.0                 ┆ 8.0                 │\n",
      "└───────────────────┴─────────────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb86ee9",
   "metadata": {},
   "source": [
    "Naturalmente, este tipo de conversiones no están exentas de eventuales problemas. Si tratamos de convertir una columna que contiene strings a valores numéricos, y dicha columna posee algún valor que no es posible de convertir en un número, **<font color=\"mediumorchid\">Polars</font>** levantará igualmente un error (de tipo `ComputeError`), informándonos la razón por la cual ésto ha ocurrido. Como antes, seteando `strict=False`, permitirá que **<font color=\"mediumorchid\">Polars</font>** ignore tales errores y transforme cualquier valor \"problemático\" en un `null`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "79327202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro DataFrame para testear el caso anterior.\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"integers\": [1, 2, 3, 4, 5],\n",
    "        \"float\": [4.0, 5.03, 6.0, 7.0, 8.0],\n",
    "        \"strings\": [\"cuatro\", \"5.0\", \"6.0\", \"siete\", \"8.0\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ecee525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strict conversion from `str` to `f32` failed for value(s) [\"cuatro\", \"siete\"]; if you were trying to cast Utf8 to temporal dtypes, consider using `strptime`\n"
     ]
    }
   ],
   "source": [
    "# La conversión de la columna \"strings\" a números levamtará una excepción.\n",
    "try:\n",
    "    result = df.select([pl.col(\"strings\").cast(pl.Float32)])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "14591ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... pero la podemos \"evitar\" seteando el parámetro strict=False.\n",
    "result = df.select([pl.col(\"strings\").cast(pl.Float32, strict=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb470039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 1)\n",
      "┌─────────┐\n",
      "│ strings │\n",
      "│ ---     │\n",
      "│ f32     │\n",
      "╞═════════╡\n",
      "│ null    │\n",
      "│ 5.0     │\n",
      "│ 6.0     │\n",
      "│ null    │\n",
      "│ 8.0     │\n",
      "└─────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cac007",
   "metadata": {},
   "source": [
    "#### Caso 3: Conversiones de fechas.\n",
    "Como ocurre en **<font color=\"mediumorchid\">Pandas</font>** con el objeto `pandas.Timestamp` especializado en el tratamiento de datos temporales (o *timestamps*), en **<font color=\"mediumorchid\">Polars</font>** es posible igualmente manipular data temporal por medio de tipos especializados, basados en los existentes en Arrow, como `Date` (para data con granularidad mínima de días) o `Datetime` (para data con granularidad mínima de microsegundos). Por lo tanto, es posible generar conversiones de tipo entre datos numéricos y *timestamps*, aprovechando que **<font color=\"mediumorchid\">Polars</font>** es compatible, al igual que **<font color=\"mediumorchid\">Pandas</font>**, con representaciones de tiempo propias de las librerías nativas **<font color=\"mediumorchid\">Datetime</font>** y **<font color=\"mediumorchid\">Dateutils</font>**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e1a0eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e13ffd",
   "metadata": {},
   "source": [
    "Vamos a profundizar en el manejo de series de tiempo en **<font color=\"mediumorchid\">Polars</font>** más adelante. Sin embargo, para el siguiente ejemplo, haremos uso de algunas funciones propias del manejo de data temporal en esta librería. En particular, construiremos un DataFrame compuesto por dos secuencias regulares de datos temporales usando la función `pl.date_range()` para tales efectos. Esta función permite retornar una secuencia temporal regular entre los timestamps `start` y `end`. Usaremos además el parámetro Booleano `eager`, seteándolo en `False`, para que la función retorne una serie en vez de una expresión de **<font color=\"mediumorchid\">Polars</font>**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9668ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un DataFrame a partir de dos secuencias temporales regulares.\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"date\": pl.date_range(start=date(2023, 1, 1), end=date(2023, 1, 5), eager=True),\n",
    "        \"datetime\": pl.date_range(\n",
    "            datetime(2023, 1, 1), datetime(2023, 1, 5), eager=True\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49713cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌────────────┬─────────────────────┐\n",
      "│ date       ┆ datetime            │\n",
      "│ ---        ┆ ---                 │\n",
      "│ date       ┆ datetime[μs]        │\n",
      "╞════════════╪═════════════════════╡\n",
      "│ 2023-01-01 ┆ 2023-01-01 00:00:00 │\n",
      "│ 2023-01-02 ┆ 2023-01-02 00:00:00 │\n",
      "│ 2023-01-03 ┆ 2023-01-03 00:00:00 │\n",
      "│ 2023-01-04 ┆ 2023-01-04 00:00:00 │\n",
      "│ 2023-01-05 ┆ 2023-01-05 00:00:00 │\n",
      "└────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos este DataFrame en pantalla.\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c6be5",
   "metadata": {},
   "source": [
    "Vemos pues que los formatos de tiempo que muestra nuestro DataFrame (que, recordemos, están basados en los tipos `Date` y `Datetime` de Arrow) son iguales, en presentación, a los *timestamps* que aprendimos a utilizar en **<font color=\"mediumorchid\">Polars</font>**.\n",
    "\n",
    "Podemos utilizar el método `cast()` para realizar conversiones de estos tipos a otros de naturaleza numérica, debido a que `Date` y `Datetime` son casos particulares de enteros de 64 bits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5859d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos una conversión a tipos numéricos.\n",
    "result = df.select(\n",
    "    [\n",
    "        pl.col(\"date\").cast(pl.Int64).alias(\"date_a_int64\"), \n",
    "        pl.col(\"datetime\").cast(pl.Int64).alias(\"datetime_a_int64\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4e32810c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌──────────────┬──────────────────┐\n",
      "│ date_a_int64 ┆ datetime_a_int64 │\n",
      "│ ---          ┆ ---              │\n",
      "│ i64          ┆ i64              │\n",
      "╞══════════════╪══════════════════╡\n",
      "│ 19358        ┆ 1672531200000000 │\n",
      "│ 19359        ┆ 1672617600000000 │\n",
      "│ 19360        ┆ 1672704000000000 │\n",
      "│ 19361        ┆ 1672790400000000 │\n",
      "│ 19362        ┆ 1672876800000000 │\n",
      "└──────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f050a",
   "metadata": {},
   "source": [
    "Podemos además realizar conversiones entre strings y tipos como `Date` y `Datetime`, haciendo uso de los métodos `strftime()` y `strptime()`, los que además requieren especificar el formato de fechas a utilizar. Tal formato tiene una sintaxis denominada **chrono-format**, y que se puede consultar en la correspondiente [documentación](https://docs.rs/chrono/latest/chrono/format/strftime/index.html). Algunos códigos de frecuencia asociados a este formato se detallan en la Tabla (2.1).\n",
    "\n",
    "</p> <p style=\"text-align: center;\">Tabla (2.1): Algunos códigos de frecuencia usados para especificar el formato de una fecha o timestamp en <font color=\"mediumorchid\">Polars</font></p>\n",
    "\n",
    "| Código de frecuencia | Ejemplo  | Descripción                                                          |\n",
    "| :------------------- | :------- | :------------------------------------------------------------------- |\n",
    "| `%Y`                 | `2021`   | Año mostrado como un número entero de cuatro dígitos.                |\n",
    "| `%m`                 | `08`     | Mes codificado como un número, desde `01` a `12` (dos dígitos).      |\n",
    "| `%b`                 | `Aug`    | Mes codificado como un string, de tres letras (idioma inglés).       |\n",
    "| `%B`                 | `August` | Mes codificado como un string, con todas sus letras (idioma inglés). |\n",
    "| `%d`                 | `18`     | Día codificado como un número, desde `01` a `31` (dos dígitos).      |\n",
    "| `%H`                 | `08`     | Hora codificada como un número desde `00` a `23` (dos dígitos).      |\n",
    "| `%M`                 | `32`     | Minuto codificado como un número desde `00` a `59` (dos dígitos).    |\n",
    "| `%S`                 | `48`     | Segundo codificado como un número desde `00` a `60` (dos dígitos).   |\n",
    "\n",
    "En general, usamos únicamente `strftime()` para generar conversiones entre strings y timestamps en **<font color=\"mediumorchid\">Polars</font>**. El uso de `strptime()` es muy parecido, salvo que éste último método nos entrega más opciones relativas a cambios en zonas horarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f4c27f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un DataFrame que usaremos para testear los cambios de tipo entre strings y timestamps.\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"date\": pl.date_range(start=date(2023, 8, 1), end=date(2023, 8, 5), eager=True),\n",
    "        \"string\": [\n",
    "            \"2023-08-01\",\n",
    "            \"2023-08-02\",\n",
    "            \"2023-08-03\",\n",
    "            \"2023-08-04\",\n",
    "            \"2023-08-05\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "46a28b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos cambios de fotmato mediante el método strftime().\n",
    "result = df.select(\n",
    "    [\n",
    "        pl.col(\"date\").dt.strftime(\"%Y-%m-%d\").alias(\"formateo_a_date\"),\n",
    "        pl.col(\"string\").str.strptime(pl.Datetime, \"%Y-%m-%d\").alias(\"formateo_a_datetime\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "11e0e462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌─────────────────┬─────────────────────┐\n",
      "│ formateo_a_date ┆ formateo_a_datetime │\n",
      "│ ---             ┆ ---                 │\n",
      "│ str             ┆ datetime[μs]        │\n",
      "╞═════════════════╪═════════════════════╡\n",
      "│ 2023-08-01      ┆ 2023-08-01 00:00:00 │\n",
      "│ 2023-08-02      ┆ 2023-08-02 00:00:00 │\n",
      "│ 2023-08-03      ┆ 2023-08-03 00:00:00 │\n",
      "│ 2023-08-04      ┆ 2023-08-04 00:00:00 │\n",
      "│ 2023-08-05      ┆ 2023-08-05 00:00:00 │\n",
      "└─────────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172759d0",
   "metadata": {},
   "source": [
    "### Evaluación de strings.\n",
    "Es posible hacer uso de expresiones de **<font color=\"mediumorchid\">Polars</font>** para construir evaluaciones de strings en las columnas de un DataFrame, siempre que tales columnas tengan un tipo `Utf-8`, que es el más común (y utilizado) de los tipos a la hora de manipular strings en una estructura de datos de **<font color=\"mediumorchid\">Polars</font>**: La manipulación de strings puede ser un tanto ineficiente debido a que su tamaño en la memoria de nuestro computador es esencialmente impredecible (lo que provoca que la CPU deba acceder a una cantidad enorme de localizaciones aleatorias en la memoria). El uso del backend de Arrow en **<font color=\"mediumorchid\">Polars</font>** para evaluar strings permite evitar este problema, ya que Arrow aloja todos los strings en un DataFrame en un bloque continuo de memoria, consiguiendo así una manipulación eficiente de datos de este tipo. Para ello, usamos el evaluador `str`, el cual se aplica sobre cualquier expresión de **<font color=\"mediumorchid\">Polars</font>** en un contexto dado.\n",
    "\n",
    "Vamos a probar algunos evaluadores haciendo uso de un pequeño dataset relativo a dos semanas de comentarios realizados en relación rechazos de operadores a las recomendaciones realizadas por un sistema de optimización de cobre fino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3a09615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accedemos al dataset.\n",
    "df_recom = pl.read_csv(source=\"datasets/rejections_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e927ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 4)\n",
      "┌──────────────────┬─────────────────────────────────┬───────────┬─────────────────────────────────┐\n",
      "│ fecha            ┆ variable_recomendada            ┆ estado    ┆ comentarios                     │\n",
      "│ ---              ┆ ---                             ┆ ---       ┆ ---                             │\n",
      "│ str              ┆ str                             ┆ str       ┆ str                             │\n",
      "╞══════════════════╪═════════════════════════════════╪═══════════╪═════════════════════════════════╡\n",
      "│ 2023/04/01 08:00 ┆ dosificacion_colector_espumante ┆ rechazada ┆ reactivo con problemas de       │\n",
      "│                  ┆                                 ┆           ┆ disolu…                         │\n",
      "│ 2023/04/01 20:00 ┆ dosificacion_colector_primario  ┆ rechazada ┆ pulpa con exceso de magnetita   │\n",
      "│ 2023/04/02 08:00 ┆ dosificacion_colector_secundari ┆ rechazada ┆ valor en contrastacion en       │\n",
      "│                  ┆ o                               ┆           ┆ terren…                         │\n",
      "│ 2023/04/02 20:00 ┆ dosificacion_colector_primario  ┆ rechazada ┆ reactivo ya se encuentra con    │\n",
      "│                  ┆                                 ┆           ┆ val…                            │\n",
      "│ 2023/04/03 08:00 ┆ dosificacion_colector_primario  ┆ rechazada ┆ sensor de nivel en tks          │\n",
      "│                  ┆                                 ┆           ┆ descalibr…                      │\n",
      "└──────────────────┴─────────────────────────────────┴───────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas del dataset en pantalla.\n",
    "print(df_recom.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603c902",
   "metadata": {},
   "source": [
    "Un ejemplo de evaluador es `n_chars()`, que nos permite determinar el número de caracteres asociados a una fila en una columna dada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d305bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de caracteres por fila en cada columna.\n",
    "result = df_recom.select(\n",
    "    [\n",
    "        pl.col(\"comentarios\").str.n_chars().alias(\"n_caract_comentarios\"),\n",
    "        pl.col(\"variable_recomendada\").str.n_chars().alias(\"n_caract_variable_recomendada\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19a3d571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌──────────────────────┬───────────────────────────────┐\n",
      "│ n_caract_comentarios ┆ n_caract_variable_recomendada │\n",
      "│ ---                  ┆ ---                           │\n",
      "│ u32                  ┆ u32                           │\n",
      "╞══════════════════════╪═══════════════════════════════╡\n",
      "│ 70                   ┆ 31                            │\n",
      "│ 29                   ┆ 30                            │\n",
      "│ 33                   ┆ 32                            │\n",
      "│ 55                   ┆ 30                            │\n",
      "│ 35                   ┆ 30                            │\n",
      "└──────────────────────┴───────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas del resultado anterior.\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df80d56",
   "metadata": {},
   "source": [
    "Podemos chequear la presencia de un determinado caracter en una determinada fila de una columna de tipo string por medio del evaluador `contains()`, el cual acepta como argumento un determinado *sub-string* de interés, o bien, alguna expresión regular (*regex*). Si el patrón que buscamos debe ubicarse al inicio o al final del string, podemos usar alternativamente los evaluadores `starts_with()` y `ends_with()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "10775fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos algunos evaluadores para buscar patrones.\n",
    "result = df_recom.select(\n",
    "    [\n",
    "        pl.col(\"comentarios\"),\n",
    "        pl.col(\"comentarios\").str.contains(\"contrastacion|anomalo\").alias(\"regex\"),\n",
    "        pl.col(\"comentarios\").str.contains(\"problemas\").alias(\"literal\"),\n",
    "        pl.col(\"comentarios\").str.starts_with(\"valor\").alias(\"begins_with\"),\n",
    "        pl.col(\"comentarios\").str.ends_with(\"terreno\").alias(\"ends_with\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b71f5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 5)\n",
      "┌───────────────────────────────────┬───────┬─────────┬─────────────┬───────────┐\n",
      "│ comentarios                       ┆ regex ┆ literal ┆ begins_with ┆ ends_with │\n",
      "│ ---                               ┆ ---   ┆ ---     ┆ ---         ┆ ---       │\n",
      "│ str                               ┆ bool  ┆ bool    ┆ bool        ┆ bool      │\n",
      "╞═══════════════════════════════════╪═══════╪═════════╪═════════════╪═══════════╡\n",
      "│ reactivo con problemas de disolu… ┆ false ┆ true    ┆ false       ┆ false     │\n",
      "│ pulpa con exceso de magnetita     ┆ false ┆ false   ┆ false       ┆ false     │\n",
      "│ valor en contrastacion en terren… ┆ true  ┆ false   ┆ true        ┆ true      │\n",
      "│ reactivo ya se encuentra con val… ┆ false ┆ false   ┆ false       ┆ false     │\n",
      "│ sensor de nivel en tks descalibr… ┆ false ┆ false   ┆ false       ┆ false     │\n",
      "└───────────────────────────────────┴───────┴─────────┴─────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas del resultado anterior.\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da183ed",
   "metadata": {},
   "source": [
    "Otros evaluadores utilizados con regularidad son `replace()` y `replace_all()`, que permiten realizar reemplazos de un patrón por otro en una determinada columna de tipo string, ya sea considerando una ocurrencia del patrón a reemplazar (`replace()`), o de todas ellas (`replace_all()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a401a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos algunos reemplazos de patrones por otros.\n",
    "result = df_recom.select(\n",
    "    [\n",
    "        pl.col(\"comentarios\").alias(\"original\"),\n",
    "        pl.col(\"comentarios\").str.replace(pattern=r\"pulpa\\b\", value=\"PULPA\").alias(\"replace\"),\n",
    "        pl.col(\"comentarios\")\n",
    "        .str\n",
    "        .replace_all(pattern=\"reactivo\", value=\"colector\", literal=True)\n",
    "        .alias(\"replace_all\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7f12fe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌────────────────────────────────┬────────────────────────────────┬────────────────────────────────┐\n",
      "│ original                       ┆ replace                        ┆ replace_all                    │\n",
      "│ ---                            ┆ ---                            ┆ ---                            │\n",
      "│ str                            ┆ str                            ┆ str                            │\n",
      "╞════════════════════════════════╪════════════════════════════════╪════════════════════════════════╡\n",
      "│ reactivo con problemas de      ┆ reactivo con problemas de      ┆ colector con problemas de      │\n",
      "│ disolu…                        ┆ disolu…                        ┆ disolu…                        │\n",
      "│ pulpa con exceso de magnetita  ┆ PULPA con exceso de magnetita  ┆ pulpa con exceso de magnetita  │\n",
      "│ valor en contrastacion en      ┆ valor en contrastacion en      ┆ valor en contrastacion en      │\n",
      "│ terren…                        ┆ terren…                        ┆ terren…                        │\n",
      "│ reactivo ya se encuentra con   ┆ reactivo ya se encuentra con   ┆ colector ya se encuentra con   │\n",
      "│ val…                           ┆ val…                           ┆ val…                           │\n",
      "│ sensor de nivel en tks         ┆ sensor de nivel en tks         ┆ sensor de nivel en tks         │\n",
      "│ descalibr…                     ┆ descalibr…                     ┆ descalibr…                     │\n",
      "└────────────────────────────────┴────────────────────────────────┴────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas del resultado anterior.\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2fabb",
   "metadata": {},
   "source": [
    "Otro evaluador muy usado en **<font color=\"mediumorchid\">Polars</font>** es `to_datetime()`, que nos permite convertir cualquier columna con strings a fechas, siempre que tales strings efectivamente representen datos de este tipo. Cualquier valor no inferible como `Date` o `Datetime` por **<font color=\"mediumorchid\">Polars</font>** provocará errores en una conversión de este tipo, salvo que se setee el parámetro `strict=False` a fin de llevar cualquiera de esos valores problemáticos a `null`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "001cc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión de string a fechas por medio de evaluador to_datetime().\n",
    "result = df_recom.select([pl.col(\"fecha\").str.to_datetime()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6ee56166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 1)\n",
      "┌─────────────────────┐\n",
      "│ fecha               │\n",
      "│ ---                 │\n",
      "│ datetime[μs]        │\n",
      "╞═════════════════════╡\n",
      "│ 2023-04-01 08:00:00 │\n",
      "│ 2023-04-01 20:00:00 │\n",
      "│ 2023-04-02 08:00:00 │\n",
      "│ 2023-04-02 20:00:00 │\n",
      "│ 2023-04-03 08:00:00 │\n",
      "└─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas del resultado anterior.\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e422cb6",
   "metadata": {},
   "source": [
    "### Agregaciones y ordenamientos.\n",
    "\n",
    "Anteriormente, revisamos la construcción de agregaciones en **<font color=\"mediumorchid\">Polars</font>** por medio de agrupamientos bajo el contexto `groupby()`. Sin embargo, también es posible construir agregaciones directamente por medio del uso de expresiones de **<font color=\"mediumorchid\">Polars</font>**. Para ejemplificar este tipo de operaciones, vamos a a hacer uso de un dataset extendido relativo a los pilares de un nivel de producción de una mina subterránea que analizamos previamente. Dicho dataset se ha almacenado en el archivo `datasets/pillars_geotechnical_data.csv`, y lo cargaremos a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d8ee3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos nuestro dataset en un DataFrame de Polars.\n",
    "df_pillars = pl.read_csv(source=\"datasets/pillars_geotechnical_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "15e9e15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id_pilar</th><th>x</th><th>y</th><th>z</th><th>heterogeneidad_geologica</th><th>litologia_dominante</th><th>proximadad_de_fallas</th><th>condicion_de_frente</th><th>num_singularidades_geometricas</th><th>area_pilar</th><th>frec_fracturas</th><th>altura_extraible_promedio</th><th>carga_vertical_pre_mineria</th><th>carga_vertical_transicion</th><th>estado_pilar</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>776.718035</td><td>259.185135</td><td>1125.0</td><td>&quot;homogenea&quot;</td><td>&quot;complejos de r…</td><td>1</td><td>&quot;concavo&quot;</td><td>0</td><td>286.5643</td><td>1.979</td><td>264</td><td>11.126392</td><td>43.236799</td><td>0</td></tr><tr><td>2</td><td>811.143435</td><td>263.059235</td><td>1125.0</td><td>&quot;muy heterogene…</td><td>&quot;brecha de anhi…</td><td>1</td><td>&quot;concavo&quot;</td><td>0</td><td>286.5643</td><td>3.07</td><td>262</td><td>9.646313</td><td>38.197458</td><td>1</td></tr><tr><td>3</td><td>845.568835</td><td>266.933335</td><td>1125.0</td><td>&quot;homogenea&quot;</td><td>&quot;brecha de anhi…</td><td>1</td><td>&quot;concavo&quot;</td><td>0</td><td>286.5643</td><td>3.07</td><td>261</td><td>10.590594</td><td>41.899365</td><td>1</td></tr><tr><td>4</td><td>879.994235</td><td>270.807435</td><td>1125.0</td><td>&quot;homogenea&quot;</td><td>&quot;brecha de anhi…</td><td>1</td><td>&quot;plano&quot;</td><td>0</td><td>286.5643</td><td>1.75</td><td>260</td><td>10.605144</td><td>42.21432</td><td>1</td></tr><tr><td>5</td><td>914.419635</td><td>274.681535</td><td>1125.0</td><td>&quot;homogenea&quot;</td><td>&quot;brecha de anhi…</td><td>0</td><td>&quot;plano&quot;</td><td>0</td><td>286.5643</td><td>0.67</td><td>259</td><td>10.059537</td><td>40.144558</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 15)\n",
       "┌──────────┬───────────┬──────────┬────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ id_pilar ┆ x         ┆ y        ┆ z      ┆ … ┆ altura_ext ┆ carga_vert ┆ carga_vert ┆ estado_pil │\n",
       "│ ---      ┆ ---       ┆ ---      ┆ ---    ┆   ┆ raible_pro ┆ ical_pre_m ┆ ical_trans ┆ ar         │\n",
       "│ i64      ┆ f64       ┆ f64      ┆ f64    ┆   ┆ medio      ┆ ineria     ┆ icion      ┆ ---        │\n",
       "│          ┆           ┆          ┆        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ i64        │\n",
       "│          ┆           ┆          ┆        ┆   ┆ i64        ┆ f64        ┆ f64        ┆            │\n",
       "╞══════════╪═══════════╪══════════╪════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 1        ┆ 776.71803 ┆ 259.1851 ┆ 1125.0 ┆ … ┆ 264        ┆ 11.126392  ┆ 43.236799  ┆ 0          │\n",
       "│          ┆ 5         ┆ 35       ┆        ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 2        ┆ 811.14343 ┆ 263.0592 ┆ 1125.0 ┆ … ┆ 262        ┆ 9.646313   ┆ 38.197458  ┆ 1          │\n",
       "│          ┆ 5         ┆ 35       ┆        ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 3        ┆ 845.56883 ┆ 266.9333 ┆ 1125.0 ┆ … ┆ 261        ┆ 10.590594  ┆ 41.899365  ┆ 1          │\n",
       "│          ┆ 5         ┆ 35       ┆        ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 4        ┆ 879.99423 ┆ 270.8074 ┆ 1125.0 ┆ … ┆ 260        ┆ 10.605144  ┆ 42.21432   ┆ 1          │\n",
       "│          ┆ 5         ┆ 35       ┆        ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 5        ┆ 914.41963 ┆ 274.6815 ┆ 1125.0 ┆ … ┆ 259        ┆ 10.059537  ┆ 40.144558  ┆ 0          │\n",
       "│          ┆ 5         ┆ 35       ┆        ┆   ┆            ┆            ┆            ┆            │\n",
       "└──────────┴───────────┴──────────┴────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos las primeras filas de este DataFrame.\n",
    "df_pillars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5dd78",
   "metadata": {},
   "source": [
    "Este DataFrame tiene las siguientes columnas:\n",
    "\n",
    "- `id_pilar`: Número identificador asociado al pilar.\n",
    "- `x`: Coordenada X del pilar en el sistema de coordenadas mina (m).\n",
    "- `y`: Coordenada Y del pilar en el sistema de coordenadas mina (m).\n",
    "- `z`: Coordenada Z del pilar en el sistema de coordenadas mina (m).\n",
    "- `heterogeneidad_geologica`: Condición asociada al número de unidades litológicas detectadas en el macizo rocoso que constituye el pilar. Una condición homogénea implica que el pilar está constituido por una única unidad litológica, mientras que a mayor cantidad de estas unidades presentes en el pilar, mayor heterogeneidad.\n",
    "- `litologia_dominante`: Unidad litológica que predomina en la constitución del pilar.\n",
    "- `proximadad_de_fallas`: Cantidad de fallas de orientación desfavorable cercanas al pilar (a menos de 100 metros).\n",
    "- `condicion_de_frente`: Condición del frente de hundimiento cuando ha estado en las inmediaciones del pilar (en una banda de influencia de 60 metros delante del mismo y 30 metros detrás).\n",
    "- `num_singularidades_geometricas`: Número de singularidades geométricas presentes en el pilar (por ejemplo, pérdidas de sección por efecto de derrumbes o labores necesarias, como puntos de vaciado).\n",
    "- `area_pilar`: Área de la sección transversal del pilar ($\\mathrm{m}^{2}$).\n",
    "- `frec_fracturas`: Frecuencia de fracturas asociada al pilar (en número de fracturas por metro lineal), y que corresponde a una estimación de la calidad geotécnica del macizo rocoso que constituye cada pilar (1/m).\n",
    "- `altura_extraible_promedio`: Altura extraible promedio de los puntos de extracción adyacentes al pilar (m).\n",
    "- `carga_vertical_pre_mineria`: Carga vertical pre-minería (en MPa) estimada sobre el pilar mediante modelamiento numérico de esfuerzos.\n",
    "- `carga_vertical_transicion`: Carga vertical amplificada por efecto de la actividad minera y las inmediaciones del frente de hundimiento.\n",
    "- `estado_pilar`: Categoría asociada al estado del pilar. Si su valor es igual a 1, el pilar está dañado por siniestralidades geomecánicas (colapsos). Si su valor es igual a 0, se encuentra operativo.\n",
    "\n",
    "Usaremos expresiones para construir una *query* (o consulta) basada en la siguiente agregación desde nuestro DataFrame: Queremos agrupar los colapsos en términos de la condición del frente de hundimiento documantada con respecto a su progresión relativa a la posición del pilar, para lo cual haremos uso del contexto `groupby()` para definir la columna respecto de la cual agruparemos nuestros datos (en este caso, `condicion_de_frente`), usando el contexto de agregación `agg()`. Por supuesto, queremos ordenar el resultado de esta *query* de manera descendente, para lo cual aplicaremos dicho ordenamiento usando el método `sort()` sobre la expresión resultante (seteando el parámetro `descending=True` para indicar a **<font color=\"mediumorchid\">Polars</font>** que el ordenamiento será descendente), y limitaremos los resultados a solamente 5 filas usando el método `limit()`.\n",
    "\n",
    "Además, haremos uso de la **lazy API** de **<font color=\"mediumorchid\">Polars</font>** para aumentar enormemente la eficiencia de esya *query*. No es necesario hacer esto en un DataFrame tan pequeño (con menos de 1000 filas). Pero es una muy buena práctica en general. Para transformar un DataFrame de **<font color=\"mediumorchid\">Polars</font>** en un LazyFrame, usamos el método `lazy()`. La transformación inversa, de LazyFrame a DataFrame, se realiza por medio del método `collect()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a21ad1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos una \"query\" donde agregamos pilares colapsados conforme su condición de frente.\n",
    "# A fin de aprovechar la velocidad de la lazy API, llevamos el DataFrame a un LazyFrame.\n",
    "q = (\n",
    "    df_pillars.lazy()\n",
    "    .groupby(\"condicion_de_frente\")\n",
    "    .agg(\n",
    "        [\n",
    "            (pl.col(\"estado_pilar\") == 1).sum().alias(\"colapsos\"),\n",
    "            (pl.col(\"estado_pilar\") == 0).sum().alias(\"no colapsos\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort(\"colapsos\", descending=True)\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6460f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos el resultado de la query a un DataFrame.\n",
    "result = q.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6966de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────────────────┬──────────┬─────────────┐\n",
      "│ condicion_de_frente ┆ colapsos ┆ no colapsos │\n",
      "│ ---                 ┆ ---      ┆ ---         │\n",
      "│ str                 ┆ u32      ┆ u32         │\n",
      "╞═════════════════════╪══════════╪═════════════╡\n",
      "│ plano               ┆ 135      ┆ 225         │\n",
      "│ concavo             ┆ 118      ┆ 230         │\n",
      "│ convexo             ┆ 80       ┆ 106         │\n",
      "│ muy concavo         ┆ 11       ┆ 16          │\n",
      "│ muy convexo         ┆ 0        ┆ 2           │\n",
      "└─────────────────────┴──────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55917ba0",
   "metadata": {},
   "source": [
    "De la *query* anterior, observamos que la mayoría de los colapsos han ocurrido en una condición de frente plano. Esto no necesariamente implica causalidad, pero es un muy buen punto de partida para estudiar este fenómeno en este sector productivo.\n",
    "\n",
    "Y así de fácil es construir agregaciones de alta complejidad. Podemos agregar más combinaciones de columnas categóricas por medio de la inclusión de listas con el nombre de esas columnas en el método `groupby()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "741b38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una agregación similar, pero está vez considerando combinaciones de condiciones de frente\n",
    "# proximidad de fallas y heterogeneidad geológica.\n",
    "groups = [\"condicion_de_frente\", \"proximadad_de_fallas\", \"litologia_dominante\"]\n",
    "q = (\n",
    "    df_pillars.lazy()\n",
    "    .groupby(groups)\n",
    "    .agg(\n",
    "        [\n",
    "            (pl.col(\"estado_pilar\") == 1).sum().alias(\"colapsos\"),\n",
    "            (pl.col(\"estado_pilar\") == 0).sum().alias(\"no colapsos\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort(\"colapsos\", descending=True)\n",
    "    .limit(8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cbce914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos el resultado de la query a un DataFrame.\n",
    "result = q.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2fdc6121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (8, 5)\n",
      "┌─────────────────────┬──────────────────────┬────────────────────────────┬──────────┬─────────────┐\n",
      "│ condicion_de_frente ┆ proximadad_de_fallas ┆ litologia_dominante        ┆ colapsos ┆ no colapsos │\n",
      "│ ---                 ┆ ---                  ┆ ---                        ┆ ---      ┆ ---         │\n",
      "│ str                 ┆ i64                  ┆ str                        ┆ u32      ┆ u32         │\n",
      "╞═════════════════════╪══════════════════════╪════════════════════════════╪══════════╪═════════════╡\n",
      "│ plano               ┆ 1                    ┆ complejos de rocas maficas ┆ 53       ┆ 58          │\n",
      "│ concavo             ┆ 2                    ┆ complejos de rocas maficas ┆ 41       ┆ 14          │\n",
      "│ concavo             ┆ 1                    ┆ complejos de rocas maficas ┆ 36       ┆ 80          │\n",
      "│ convexo             ┆ 1                    ┆ complejos de rocas maficas ┆ 29       ┆ 22          │\n",
      "│ plano               ┆ 2                    ┆ complejos de rocas maficas ┆ 25       ┆ 17          │\n",
      "│ convexo             ┆ 2                    ┆ complejos de rocas maficas ┆ 23       ┆ 7           │\n",
      "│ plano               ┆ 0                    ┆ complejos de rocas maficas ┆ 21       ┆ 33          │\n",
      "│ plano               ┆ 3                    ┆ complejos de rocas maficas ┆ 12       ┆ 15          │\n",
      "└─────────────────────┴──────────────────────┴────────────────────────────┴──────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70824295",
   "metadata": {},
   "source": [
    "Podemos observar que, conforme esta agregación, se tiene que los colapsos efectivamente han tendido a ocurrir cuando el frente ha presentado una geometría plana, en macizos rocosos en presencia de rocas máficas y de, al menos, una falla cercana (a menos de 100 metros de distancia del pilar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335586c",
   "metadata": {},
   "source": [
    "### Manejo de entradas vacías.\n",
    "\n",
    "#### Valores `null` y `NaN`.\n",
    "Cada columna en un DataFrame de **<font color=\"mediumorchid\">Polars</font>**, como comentamos al inicio de estos apuntes, corresponde a una serie cuya estructura está basada en un arreglo propio de Arrow. Por lo tanto, las **entradas vacías** (o **datos faltantes**) se representan por medio de un valor centinela denominado `null`. Este valor se aplica para columnas de cualquier tipo en un DataFrame, sean numéricas, de strings, listas, de timestamps, o cualquier otra.\n",
    "\n",
    "En **<font color=\"mediumorchid\">Numpy</font>** y **<font color=\"mediumorchid\">Pandas</font>**. es común representar la data faltante por medio del valor centinela `nan`, el cual significa literalmente **not a number**. En **<font color=\"mediumorchid\">Polars</font>** también existe esta representación –y se escribe en este caso como `NaN`–, pero a diferencia de las librerías anteriores, `NaN` sólo representa valores faltantes en columnas de tipo flotante, y por consiguiente **<font color=\"mediumorchid\">Polars</font>** asumirá siempre que, a pesar de su significado, representan datos de este tipo. Esto tiene consecuencias drásticamente diferentes a las que podemos encontrar en **<font color=\"mediumorchid\">Pandas</font>**, y que discutiremos un poco más adelante.\n",
    "\n",
    "Siempre es posible definir manualmente un valor faltante en **<font color=\"mediumorchid\">Polars</font>**. Para ello, bastará con hacer uso del valor `None`, el cual será inmediatamente interpretado como un `null`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e0cef508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición manual de data faltante.\n",
    "df = pl.DataFrame({\"x1\": [1, None, None, -4], \"x2\": [None, 0, -2, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e08a4880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (4, 2)\n",
      "┌──────┬──────┐\n",
      "│ x1   ┆ x2   │\n",
      "│ ---  ┆ ---  │\n",
      "│ i64  ┆ i64  │\n",
      "╞══════╪══════╡\n",
      "│ 1    ┆ null │\n",
      "│ null ┆ 0    │\n",
      "│ null ┆ -2   │\n",
      "│ -4   ┆ 1    │\n",
      "└──────┴──────┘\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos el DataFrame anterior.\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de15ce5",
   "metadata": {},
   "source": [
    "#### Cuantía de entradas de tipo `null`.\n",
    "Cada arreglo de Arrow utilizado por **<font color=\"mediumorchid\">Polars</font>** almacena dos tipos de metadatos asociados a la data faltante. Estos metadatos le permiten a **<font color=\"mediumorchid\">Polars</font>** obtener rápidamente una cuantía de `null` y cuáles entradas representan este tipo de datos.\n",
    "\n",
    "La cuantía total de entradas vacías por columna en un DataFrame puede obtenerse por medio del método `null_count()`. Este método nos retornará otro DataFrame, con la misma cantidad de columnas, y cuyos valores corresponderán a la cuantía de `null` en cada una de ellas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d0651804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuantía total de entradas vacías en nuestro DataFrame.\n",
    "null_count_df = df.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1e247229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 2)\n",
      "┌─────┬─────┐\n",
      "│ x1  ┆ x2  │\n",
      "│ --- ┆ --- │\n",
      "│ u32 ┆ u32 │\n",
      "╞═════╪═════╡\n",
      "│ 2   ┆ 1   │\n",
      "└─────┴─────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(null_count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2781b9e5",
   "metadata": {},
   "source": [
    "El método `null_count()` puede aplicarse sobre cualquier estructura de datos de **<font color=\"mediumorchid\">Polars</font>**, ya sea una serie o un DataFrame, y se trata de un método que computacionalmente no tiene no tiene costo alguno, ya que al ser la primera pieza de los metadatos asociados a la data faltante en el correspondiente arreglo de Arrow, siempre se calcula *tras bambalinas*.\n",
    "\n",
    "La segunda pieza de metadatos asociada al almacenamiento de datos faltantes corresponde al **mapa de bits validador** (*validity bitmap*), y su trabajo es indicar si un valor en una columna es, valga la redundancia, válido (es decir, representa un dato o valor definido) o no (es decir, corresponde a un dato faltante o `null`). Se llama *mapa de bits* porque se codifica bit a bit y, por tanto, resulta ser eficiente en el contexto de la manipulación de datos, ya que cada valor se codifica como un 0 o un 1. Este mapa de bits es utilizado por el método `is_null()`, el cual retorna una serie de **<font color=\"mediumorchid\">Polars</font>** que codifica cada posición en una determinada columna como `True` o `False`, dependiendo si el correspondiente valor es `null` o no:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b7678546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos un DataFrame que permite enmascarar los nulls en una columna dada.\n",
    "nulls_bitmap = df.select(\n",
    "    [\n",
    "        pl.col(\"x1\").is_null().alias(\"x1_nulls\"),\n",
    "        pl.col(\"x2\").is_null().alias(\"x2_nulls\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2ef739bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (4, 2)\n",
      "┌──────────┬──────────┐\n",
      "│ x1_nulls ┆ x2_nulls │\n",
      "│ ---      ┆ ---      │\n",
      "│ bool     ┆ bool     │\n",
      "╞══════════╪══════════╡\n",
      "│ false    ┆ true     │\n",
      "│ true     ┆ false    │\n",
      "│ true     ┆ false    │\n",
      "│ false    ┆ false    │\n",
      "└──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(nulls_bitmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24e42d",
   "metadata": {},
   "source": [
    "#### Imputaciones de data faltante en **<font color=\"mediumorchid\">Polars</font>**.\n",
    "Cualquier tipo de data faltante (representada como `null`) en una Serie de **<font color=\"mediumorchid\">Polars</font>** puede tratarse por medio de **imputaciones automáticas** (también llamadas **autoimputaciones**) mediante el uso del método `fill_null()`. Tal método permite *llenar* los espacios ocupados por data faltante de varias maneras:\n",
    "\n",
    "- Algún valor literal (por ejemplo, cero), definido por medio del parámetro `value`.\n",
    "- Por medio de alguna **estrategia de llenado**, usando el parámetro `strategy`.\n",
    "- Por medio de expresiones de **<font color=\"mediumorchid\">Polars</font>**, que permiten llenar data faltante con valores de otra columna.\n",
    "\n",
    "También es posible aplicar interpolaciones, aunque ésto lo veremos más adelante.\n",
    "\n",
    "**CASO 1 – Imputaciones directas:** Como comentamos previamente, usamos el parámetro `value` en el método `fill_null()` para definir cualquier autoimputación con un valor estático, explícitamente definido. Una buena práctica escribir los valores en el DataFrame resultante por medio de la expresión `pl.lit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6b78d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoimputación directa, cambiando nulls por ceros.\n",
    "explicit_fill = df.with_columns(\n",
    "    [\n",
    "        (\n",
    "            pl.col(\"x1\")\n",
    "            .fill_null(value=pl.lit(0))\n",
    "            .alias(\"x1_imputed\")\n",
    "        ),\n",
    "        (\n",
    "            pl.col(\"x2\")\n",
    "            .fill_null(value=pl.lit(0))\n",
    "            .alias(\"x2_imputed\")\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "277231c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (4, 4)\n",
      "┌──────┬──────┬────────────┬────────────┐\n",
      "│ x1   ┆ x2   ┆ x1_imputed ┆ x2_imputed │\n",
      "│ ---  ┆ ---  ┆ ---        ┆ ---        │\n",
      "│ i64  ┆ i64  ┆ i64        ┆ i64        │\n",
      "╞══════╪══════╪════════════╪════════════╡\n",
      "│ 1    ┆ null ┆ 1          ┆ 0          │\n",
      "│ null ┆ 0    ┆ 0          ┆ 0          │\n",
      "│ null ┆ -2   ┆ 0          ┆ -2         │\n",
      "│ -4   ┆ 1    ┆ -4         ┆ 1          │\n",
      "└──────┴──────┴────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(explicit_fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e2223",
   "metadata": {},
   "source": [
    "**CASO 2 – Estrategias de imputación:** **<font color=\"mediumorchid\">Polars</font>** nos permite implementar varias estrategias de imputación para data faltante haciendo uso del parámetro `strategy` en el método `fill_null()`. Por ejemplo, una estrategia común es llenar los espacios de data faltante usando los valores de la fila inmediatamente anterior, lo que se conoce en la práctica como **llenado hacia adelante** o **forward fill**. Podemos realizar una imputación de este tipo seteando el parámetro `strategy=\"forward\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6c03d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoimputación con un forward fill.\n",
    "forward_fill = df.with_columns(\n",
    "    [\n",
    "        (\n",
    "            pl.col(\"x1\")\n",
    "            .fill_null(strategy=\"forward\")\n",
    "            .alias(\"x1_ffill\")\n",
    "        ),\n",
    "        (\n",
    "            pl.col(\"x2\")\n",
    "            .fill_null(strategy=\"forward\")\n",
    "            .alias(\"x2_ffill\")\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bfaac51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (4, 4)\n",
      "┌──────┬──────┬──────────┬──────────┐\n",
      "│ x1   ┆ x2   ┆ x1_ffill ┆ x2_ffill │\n",
      "│ ---  ┆ ---  ┆ ---      ┆ ---      │\n",
      "│ i64  ┆ i64  ┆ i64      ┆ i64      │\n",
      "╞══════╪══════╪══════════╪══════════╡\n",
      "│ 1    ┆ null ┆ 1        ┆ null     │\n",
      "│ null ┆ 0    ┆ 1        ┆ 0        │\n",
      "│ null ┆ -2   ┆ 1        ┆ -2       │\n",
      "│ -4   ┆ 1    ┆ -4       ┆ 1        │\n",
      "└──────┴──────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(forward_fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6428a",
   "metadata": {},
   "source": [
    "Por supuesto, existen otras estrategias:\n",
    "\n",
    "- `strategy=\"backward\"`, permite rellenar los espacios de data faltante con el valor correspondiente de la fila inmediatamente posterior. Se trata de una estrategia inversa a la adoptada en un *forward fill*, denominada por tanto **backward fill**.\n",
    "- `strategy=\"mean\"`, permite rellenar los espacios de data faltante con la media de los valores definidos en la columna respectiva.\n",
    "- `strategy=\"max\"` y `strategy=\"max\"`, permiten rellenar los espacios de data faltante con el máximo o mínimo, respectivamente, de los valores definidos en la columna respectiva.\n",
    "\n",
    "Algunos ejemplos de uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cf9cf647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoimputación con un backward fill.\n",
    "backward_fill = df.with_columns(\n",
    "    [\n",
    "        (\n",
    "            pl.col(\"x1\")\n",
    "            .fill_null(strategy=\"backward\")\n",
    "            .alias(\"x1_bfill\")\n",
    "        ),\n",
    "        (\n",
    "            pl.col(\"x2\")\n",
    "            .fill_null(strategy=\"backward\")\n",
    "            .alias(\"x2_bfill\")\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "21d4b3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (4, 4)\n",
      "┌──────┬──────┬──────────┬──────────┐\n",
      "│ x1   ┆ x2   ┆ x1_bfill ┆ x2_bfill │\n",
      "│ ---  ┆ ---  ┆ ---      ┆ ---      │\n",
      "│ i64  ┆ i64  ┆ i64      ┆ i64      │\n",
      "╞══════╪══════╪══════════╪══════════╡\n",
      "│ 1    ┆ null ┆ 1        ┆ 0        │\n",
      "│ null ┆ 0    ┆ -4       ┆ 0        │\n",
      "│ null ┆ -2   ┆ -4       ┆ -2       │\n",
      "│ -4   ┆ 1    ┆ -4       ┆ 1        │\n",
      "└──────┴──────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(backward_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "11a1dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoimputación con un llenado conforme medias.\n",
    "mean_fill = df.with_columns(\n",
    "    [\n",
    "        (\n",
    "            pl.col(\"x1\")\n",
    "            .fill_null(strategy=\"mean\")\n",
    "            .alias(\"x1_mfill\")\n",
    "        ),\n",
    "        (\n",
    "            pl.col(\"x2\")\n",
    "            .fill_null(strategy=\"mean\")\n",
    "            .alias(\"x2_mfill\")\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d85958d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (4, 4)\n",
      "┌──────┬──────┬──────────┬──────────┐\n",
      "│ x1   ┆ x2   ┆ x1_mfill ┆ x2_mfill │\n",
      "│ ---  ┆ ---  ┆ ---      ┆ ---      │\n",
      "│ i64  ┆ i64  ┆ i64      ┆ i64      │\n",
      "╞══════╪══════╪══════════╪══════════╡\n",
      "│ 1    ┆ null ┆ 1        ┆ 0        │\n",
      "│ null ┆ 0    ┆ -1       ┆ 0        │\n",
      "│ null ┆ -2   ┆ -1       ┆ -2       │\n",
      "│ -4   ┆ 1    ┆ -4       ┆ 1        │\n",
      "└──────┴──────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(mean_fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210607c9",
   "metadata": {},
   "source": [
    "**CASO 3 - Imputaciones conforme expresiones:** Para mayor flexibilidad, es posible definir expresiones arbitrarias que nos permitan construir imputaciones personalizadas. Por ejemplo, si queremos utilizar las medianas de cada columna para imputar la data faltante, podemos usar la función `pl.median()` junto con el parámetro `value`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7ed75855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoimputación con un llenado conforme medias.\n",
    "median_fill = df.with_columns(\n",
    "    [\n",
    "        (\n",
    "            pl.col(\"x1\")\n",
    "            .fill_null(value=pl.median(\"x1\"))\n",
    "            .alias(\"x1_med_fill\")\n",
    "        ),\n",
    "        (\n",
    "            pl.col(\"x2\")\n",
    "            .fill_null(value=pl.median(\"x2\"))\n",
    "            .alias(\"x2_med_fill\")\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e9595b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (4, 4)\n",
      "┌──────┬──────┬─────────────┬─────────────┐\n",
      "│ x1   ┆ x2   ┆ x1_med_fill ┆ x2_med_fill │\n",
      "│ ---  ┆ ---  ┆ ---         ┆ ---         │\n",
      "│ i64  ┆ i64  ┆ f64         ┆ f64         │\n",
      "╞══════╪══════╪═════════════╪═════════════╡\n",
      "│ 1    ┆ null ┆ 1.0         ┆ 0.0         │\n",
      "│ null ┆ 0    ┆ -1.5        ┆ 0.0         │\n",
      "│ null ┆ -2   ┆ -1.5        ┆ -2.0        │\n",
      "│ -4   ┆ 1    ┆ -4.0        ┆ 1.0         │\n",
      "└──────┴──────┴─────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(median_fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e1166",
   "metadata": {},
   "source": [
    "#### Tratamiento de `NaN`.\n",
    "Como habíamos comentado previamente, la definición de data faltante **<font color=\"mediumorchid\">Polars</font>** es por medio del valor `null`. No obstante, es posible hacer uso de `NaN`, de forma similar a lo que hacemos en **<font color=\"mediumorchid\">Pandas</font>**, para describir data faltante estrictamente en columnas que sean de tipo flotante. Un valor `NaN` puede definirse por medio de **<font color=\"mediumorchid\">Numpy</font>** y su objeto `numpy.nan`. Nunca debemos hacerlo por medio del valor reservado `None` de Python, ya que `None` será traducido automáticamente por **<font color=\"mediumorchid\">Polars</font>** en un `null`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7232951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición manual de data faltante.\n",
    "df = pl.DataFrame({\"x1\": [1, np.nan, np.nan, -4, 2], \"x2\": [np.nan, 0, -2, 1, 5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "15d48f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌──────┬──────┐\n",
      "│ x1   ┆ x2   │\n",
      "│ ---  ┆ ---  │\n",
      "│ f64  ┆ f64  │\n",
      "╞══════╪══════╡\n",
      "│ 1.0  ┆ NaN  │\n",
      "│ NaN  ┆ 0.0  │\n",
      "│ NaN  ┆ -2.0 │\n",
      "│ -4.0 ┆ 1.0  │\n",
      "│ 2.0  ┆ 5.0  │\n",
      "└──────┴──────┘\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos el DataFrame anterior.\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e1f5e8",
   "metadata": {},
   "source": [
    "Cuando hacemos uso de `NaN` en cualquier columna de un DataFrame, dicha columna debe ser der tipo flotante. Cuando no es así, **<font color=\"mediumorchid\">Polars</font>** levantará inmediatamente un error.\n",
    "\n",
    "Debido a que `NaN` es un valor centinela que denota la ausencia de datos numéricos sólo para el caso de columnas de tipo flotante en **<font color=\"mediumorchid\">Polars</font>** y, por consiguiente, **un valor `NaN` no se considera como data fantante por** **<font color=\"mediumorchid\">Polars</font>**. En la práctica, esto implica que:\n",
    "\n",
    "- Los valores de tipo `NaN` **no serán contados** por el método `null_count()`.\n",
    "- Los valores de tipo `NaN` **no serán rellenados** por el método `fill_null()`. En este caso, los valores de tipo `NaN` tienen su propio método, que es `fill_nan()`.\n",
    "\n",
    "Adicionalmente, **<font color=\"mediumorchid\">Polars</font>** cuenta con el método `is_nan()` para detectar las posiciones asociadas a valores de tipo `NaN`. Debido a que los arreglos de Arrow que soportan *tras bambalinas* las columnas de un DataFrame no cuentan con un bitmap validador para `NaN`, el cómputo de las posiciones asociadas a estos datos no es parte de los metadatos de un DataFrame y, por extensión, resulta más costoso que detectar `null` con el método `is_null()`.\n",
    "\n",
    "Finalmente, en **<font color=\"mediumorchid\">Pandas</font>**, la ocurrencia de `NaN` no genera ningún problema a la hora de hacer cálculos con la data almacenada en una columna que contenga estos valores (por ejemplo, promedios o cualquier tipo de agregaciones). Sin embargo, en **<font color=\"mediumorchid\">Polars</font>**, esto no es así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "76000dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intentamos calcular las medias de cada columna.\n",
    "means_df = df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8a18c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 2)\n",
      "┌─────┬─────┐\n",
      "│ x1  ┆ x2  │\n",
      "│ --- ┆ --- │\n",
      "│ f64 ┆ f64 │\n",
      "╞═════╪═════╡\n",
      "│ NaN ┆ NaN │\n",
      "└─────┴─────┘\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el resultado anterior.\n",
    "print(means_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
